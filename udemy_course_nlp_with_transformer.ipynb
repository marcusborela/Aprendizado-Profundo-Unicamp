{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcusborela/Aprendizado-Profundo-Unicamp/blob/main/udemy_course_nlp_with_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s89IwMCbNrVl"
      },
      "source": [
        "# Named Entity Recognition (NER) With SpaCy\n",
        "\n",
        "We will be performing NER on threads from the **Investing** subreddit, but first let's test SpaCy for named entity recognition (NER) using an example from */r/investing*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e-SPwZfkNrVq"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OG1btEBnNrVs",
        "outputId": "2dcf6454-eeea-47e6-9daf-2566d9a40f5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-16 19:54:13.016030: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 14.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iaAJUm3SNrVs"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YoHCrcqNNrVt"
      },
      "outputs": [],
      "source": [
        "txt = (\"Given the recent downturn in stocks especially in tech which is likely to persist as yields keep going up, \"\n",
        "       \"I thought it would be prudent to share the risks of investing in ARK ETFs, written up very nicely by \"\n",
        "       \"[The Bear Cave](https://thebearcave.substack.com/p/special-edition-will-ark-invest-blow). The risks comes \"\n",
        "       \"primarily from ARK's illiquid and very large holdings in small cap companies. ARK is forced to sell its \"\n",
        "       \"holdings whenever its liquid ETF gets hit with outflows as is especially the case in market downturns. \"\n",
        "       \"This could force very painful liquidations at unfavorable prices and the ensuing crash goes into a \"\n",
        "       \"positive feedback loop leading into a death spiral enticing even more outflows and predatory shorts.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "F4RG3P3INrVu"
      },
      "outputs": [],
      "source": [
        "doc = nlp(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "81jive9oNrVv",
        "outputId": "3753ba29-7bf0-420d-9f77-28fc8bf313c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Given the recent downturn in stocks especially in tech which is likely to persist as yields keep going up, I thought it would be prudent to share the risks of investing in \\n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\\n    ARK ETFs\\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\\n</mark>\\n, written up very nicely by [\\n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\\n    The Bear Cave](https://thebearcave.substack.com/p\\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\\n</mark>\\n/special-edition-will-ark-invest-blow). The risks comes primarily from \\n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\\n    ARK\\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\\n</mark>\\n\\'s illiquid and very large holdings in small cap companies. \\n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\\n    ARK\\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\\n</mark>\\n is forced to sell its holdings whenever its liquid ETF gets hit with outflows as is especially the case in market downturns. This could force very painful liquidations at unfavorable prices and the ensuing crash goes into a positive feedback loop leading into a death spiral enticing even more outflows and predatory shorts.</div>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "displacy.render(doc, style='ent')\n",
        "# displacy.serve(doc, style='ent') if not running in a notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_rITjiHNrVw"
      },
      "source": [
        "Immediately we're able to produce not perfect, but pretty good NER. We are using the [`en_core_web_sm`](https://spacy.io/models/en) model - `en` referring to English and `sm` small.\n",
        "\n",
        "The model is accurately identifying ARK as an organization. It does also classify ETF (exchange traded fund) as an organization, which is not the case (an ETF is a grouping of securities on the markets), but it's easy to see why this is being classified as one. The other tag we can see is `WORK_OF_ART`, it isn't inherently clear what exactly this means, so we can get more information using `spacy.explain`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4skUvzKGNrVx",
        "outputId": "f42699f5-4fc8-4593-a40d-78018cb837db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Titles of books, songs, etc.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "spacy.explain('WORK_OF_ART')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "mF_m4R1oNrVy"
      },
      "source": [
        "And we can see that this description fits well to the tagged item, which refers to an article (although not quite a book).\n",
        "\n",
        "We have a visual output from our tagged text, but this won't be particularly useful programatically. What we need is a way to extract the relevant tags (the organizations) from our text. To do that we can use `doc.ents` which will return a list of all identified entities.\n",
        "\n",
        "Each item in this entity list contains two attributes that we are interested in, `label_` and `text`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-C-Y4viONrVy",
        "outputId": "548a5627-f617-4367-b3f5-cff9f0ce09c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORG: ARK ETFs\n",
            "ORG: The Bear Cave](https://thebearcave.substack.com/p\n",
            "ORG: ARK\n",
            "ORG: ARK\n"
          ]
        }
      ],
      "source": [
        "for entity in doc.ents:\n",
        "    print(f\"{entity.label_}: {entity.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkTYFcZKNrVz"
      },
      "source": [
        "We're almost there. Now, we need to filter out any entities that are not `ORG` entities, and append those remaining `ORG`s to an organization list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YHLGAJUINrVz",
        "outputId": "a72b46e1-cf47-471b-c6be-d25cae0c66d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ARK ETFs', 'The Bear Cave](https://thebearcave.substack.com/p', 'ARK', 'ARK']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# initialize our list\n",
        "org_list = []\n",
        "\n",
        "for entity in doc.ents:\n",
        "    # if label_ is ORG, we append text, otherwise ignore\n",
        "    if entity.label_ == 'ORG':\n",
        "        org_list.append(entity.text)\n",
        "\n",
        "org_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ffYzYQOeNrV0",
        "outputId": "2d4a8063-331f-4695-a38e-6e76f7808e29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ARK ETFs', 'ARK', 'The Bear Cave](https://thebearcave.substack.com/p']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# we don't need to see 'ARK' three times, so we use set() to remove duplicates, and then convert back to list\n",
        "org_list = list(set(org_list))\n",
        "\n",
        "org_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " txt = ( \"Apple reached an all-time high stock price of 143 dollars this January.\")"
      ],
      "metadata": {
        "id": "XDRS8b8WOJkG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "px-2GPdZOXLf"
      },
      "outputs": [],
      "source": [
        "doc = nlp(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "outputId": "b89a8bd4-d631-4e1f-d375-a28cb4e4e8ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "bNcFgmW4OXLg"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\\n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\\n    Apple\\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\\n</mark>\\n reached an all-time high stock price of \\n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\\n    143 dollars\\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\\n</mark>\\n this \\n<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\\n    January\\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\\n</mark>\\n.</div>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "displacy.render(doc, style='ent')\n",
        "# displacy.serve(doc, style='ent') if not running in a notebook"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = ( \"Apple reached an all-time high stock price of 143 dollars this January.\")\n",
        "doc = nlp(txt)\n",
        "# initialize our list\n",
        "org_list = []\n",
        "\n",
        "for entity in doc.ents:\n",
        "    # if label_ is ORG, we append text, otherwise ignore\n",
        "    if entity.label_ == 'ORG':\n",
        "        org_list.append(entity.text)\n",
        "\n",
        "org_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UsZZJuNOgJW",
        "outputId": "b81934c4-f824-49bc-bb9d-bb4411f31921"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Apple']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "text = 'Apple reached an all-time high stock price of 143 dollars this January.'\n",
        "doc = nlp(text)\n",
        "org_list = [ent.text for ent in doc.ents if ent.label_ == 'ORG']"
      ],
      "metadata": {
        "id": "M0zueqP_Oucf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "org_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfkWvS-IPK5y",
        "outputId": "2ecb4022-d7d2-4ac3-a239-afe7e875322a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Apple']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IXeBI7MzPMcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aALudGHmvmj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outro exercício Q&A"
      ],
      "metadata": {
        "id": "frbfnAfCvoCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "squad_dir = 'data\\squad'\n",
        "\n",
        "if not os.path.exists(squad_dir):\n",
        "    os.mkdir(squad_dir)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ddj3cOqavqcX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/'\n",
        "file =  'dev-v2.0.json'\n",
        "\n",
        "res = requests.get(url+file)\n",
        "# write to file in chunks\n",
        "with open(os.path.join(squad_dir, file), 'wb') as f:\n",
        "    for chunk in res.iter_content(chunk_size=40):\n",
        "        f.write(chunk)"
      ],
      "metadata": {
        "id": "AuMi6hW7wkxs"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(os.path.join(squad_dir, file), 'rb') as f:\n",
        "    squad = json.load(f)"
      ],
      "metadata": {
        "id": "EU2u8weYwY52"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize list where we will place all of our data\n",
        "new_squad = []\n",
        "\n",
        "# we need to loop through groups -> paragraphs -> qa_pairs\n",
        "for group in squad['data']:\n",
        "    for paragraph in group['paragraphs']:\n",
        "        # we pull out the context from here\n",
        "        context = paragraph['context']\n",
        "        for qa_pair in paragraph['qas']:\n",
        "            # we pull out the question\n",
        "            question = qa_pair['question']\n",
        "            # now the logic to check if we have 'answers' or 'plausible_answers'\n",
        "            answer_set = set()\n",
        "            if 'answers' in qa_pair.keys() and len(qa_pair['answers']) > 0:\n",
        "              for answ in qa_pair['answers']:\n",
        "                answer_set.add( answ['text'])\n",
        "            if 'plausible_answers' in qa_pair.keys() and len(qa_pair['plausible_answers']) > 0:\n",
        "              for answ in qa_pair['plausible_answers']:\n",
        "                answer_set.add( answ['text'])\n",
        "            # append dictionary sample to parsed squad\n",
        "            new_squad.append({\n",
        "                'question': question,\n",
        "                'answer': list(answer_set),\n",
        "                'context': context\n",
        "            })\n"
      ],
      "metadata": {
        "id": "Xy0WRQhNv9VF"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_squad[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njOk78rayXuC",
        "outputId": "40cfe7b1-b7f3-41fc-ba2e-a1db59fd1bd9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'In what country is Normandy located?',\n",
              " 'answer': ['France'],\n",
              " 'context': 'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.'}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(squad_dir, 'dev.json'), 'w') as f:\n",
        "    json.dump(new_squad, f)"
      ],
      "metadata": {
        "id": "9kv7wLm-wJEe"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In task 2, after Lecture 59, I think it would be important to group the answers in a list. This grouping should be considered in lesson 61, in the calculation of EM metric. In the calculation of EM, it is enough that it is equal to one of the answers in the list.\n",
        "\n"
      ],
      "metadata": {
        "id": "OCp8dgzx2rdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4Z7cl27s2uJV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "udemy_course_nlp-with-transformer.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}