{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcusborela/Aprendizado-Profundo-Unicamp/blob/main/Exemplo_de_uso_do_GPT_J_GPT_3_open_source.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "ZX8Z-um1L3qU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e05e66af-cdaa-464d-ae3c-628b361d25fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jun  2 11:45:40 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "FIwro5kuL_Tl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "984fb185-5ca2-425d-e270-794408b7150f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import GPTJForCausalLM\n",
        "\n",
        "import torch\n",
        "\n",
        "model = GPTJForCausalLM.from_pretrained(\n",
        "    \"EleutherAI/gpt-j-6B\", revision=\"float16\", torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "wuv4lc_ZMCSJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "929cf009-e35c-4929-c129-00470c754e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTJForCausalLM(\n",
              "  (transformer): GPTJModel(\n",
              "    (wte): Embedding(50400, 4096)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (12): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (13): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (14): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (15): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (16): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (17): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (18): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (19): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (20): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (21): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (22): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (23): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (24): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (25): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (26): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (27): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=50400, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = '''Example 1:\n",
        "Document: We don't know a lot about the effects of caffeine during pregnancy on you and your baby. So it's best to limit the amount you get each day. If you are pregnant, limit caffeine to 200 milligrams each day. This is about the amount in 1½ 8-ounce cups of coffee or one 12-ounce cup of coffee.     \n",
        "Relevant Query: Is a little caffeine ok during pregnancy?\n",
        "\n",
        "Example 2:\n",
        "Document: Passiflora herbertiana. A rare passion fruit native to Australia. Fruits are green-skinned, white fleshed, with an unknown edible rating. Some sources list the fruit as edible, sweet and tasty, while others list the fruits as being bitter and inedible.assiflora herbertiana. A rare passion fruit native to Australia. Fruits are green-skinned, white fleshed, with an unknown edible rating. Some sources list the fruit as edible, sweet and tasty, while others list the fruits as being bitter and inedible.\n",
        "Relevant Query: What fruit is native to Australia?\n",
        "\n",
        "Example 3:\n",
        "Document: The Canadian Armed Forces. 1  The first large-scale Canadian peacekeeping mission started in Egypt on November 24, 1956. 2  There are approximately 65,000 Regular Force and 25,000 reservist members in the Canadian military. 3  In Canada, August 9 is designated as National Peacekeepers' Day.\n",
        "Relevant Query: How large is the canadian military?\n",
        "\n",
        "Example 4:\n",
        "Document: The Delaware River flows through Philadelphia into the Delaware Bay. It flows through and aqueduct in the Roundout Reservoir and then flows through Philadelphia and New Jersey before emptying into the Delaware Bay.\n",
        "Relevant Query:'''\n",
        "\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "\n",
        "gen_tokens = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    temperature=0.9,\n",
        "    max_length=1024,\n",
        ")"
      ],
      "metadata": {
        "id": "QGabdIcRNc9N",
        "outputId": "7eaa0dc1-4482-4440-96cc-92eae66f0f4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Example 1:\\nDocument: We don\\'t know a lot about the effects of caffeine during pregnancy on you and your baby. So it\\'s best to limit the amount you get each day. If you are pregnant, limit caffeine to 200 milligrams each day. This is about the amount in 1½ 8-ounce cups of coffee or one 12-ounce cup of coffee.     \\nRelevant Query: Is a little caffeine ok during pregnancy?\\n\\nExample 2:\\nDocument: Passiflora herbertiana. A rare passion fruit native to Australia. Fruits are green-skinned, white fleshed, with an unknown edible rating. Some sources list the fruit as edible, sweet and tasty, while others list the fruits as being bitter and inedible.assiflora herbertiana. A rare passion fruit native to Australia. Fruits are green-skinned, white fleshed, with an unknown edible rating. Some sources list the fruit as edible, sweet and tasty, while others list the fruits as being bitter and inedible.\\nRelevant Query: What fruit is native to Australia?\\n\\nExample 3:\\nDocument: The Canadian Armed Forces. 1  The first large-scale Canadian peacekeeping mission started in Egypt on November 24, 1956. 2  There are approximately 65,000 Regular Force and 25,000 reservist members in the Canadian military. 3  In Canada, August 9 is designated as National Peacekeepers\\' Day.\\nRelevant Query: How large is the canadian military?\\n\\nExample 4:\\nDocument: The Delaware River flows through Philadelphia into the Delaware Bay. It flows through and aqueduct in the Roundout Reservoir and then flows through Philadelphia and New Jersey before emptying into the Delaware Bay.\\nRelevant Query: What is the Delaware River?\\n\\nExample 5:\\nDocument: We were invited to a party, so we invited our neighbors too. We\\'re having a barbecue with all the neighbors.\\nRelevant Query:         What are they having a barbecue for?\\n\\nExample 6:\\nDocument: Our favorite foods are eggs, bread, cheese, milk, and cheese. We like pasta.\\nRelevant Query: What types of food do you like?\\n\\nExample 7:\\nDocument: What do you drink?  I like coffee, tea, milk, and water.\\nRelevant Query: What do you like to drink?\\n\\nA:\\n\\nThere\\'s something a bit odd about \"Example 1\" and \"Example 2\" in particular. I suspect this could help in terms of the search-engines helping in a way that actually helps the searcher (which is something that the StackExchange engine doesn\\'t quite get at the moment). But that\\'s just a guess.\\n\\n<|endoftext|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_text = tokenizer.batch_decode(gen_tokens)\n",
        "gen_text"
      ],
      "metadata": {
        "id": "NrrX2jX4QtlZ",
        "outputId": "240aaf0b-3280-4012-d4ac-2e7152abb79e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Example 1:\\nDocument: We don\\'t know a lot about the effects of caffeine during pregnancy on you and your baby. So it\\'s best to limit the amount you get each day. If you are pregnant, limit caffeine to 200 milligrams each day. This is about the amount in 1½ 8-ounce cups of coffee or one 12-ounce cup of coffee.     \\nRelevant Query: Is a little caffeine ok during pregnancy?\\n\\nExample 2:\\nDocument: Passiflora herbertiana. A rare passion fruit native to Australia. Fruits are green-skinned, white fleshed, with an unknown edible rating. Some sources list the fruit as edible, sweet and tasty, while others list the fruits as being bitter and inedible.assiflora herbertiana. A rare passion fruit native to Australia. Fruits are green-skinned, white fleshed, with an unknown edible rating. Some sources list the fruit as edible, sweet and tasty, while others list the fruits as being bitter and inedible.\\nRelevant Query: What fruit is native to Australia?\\n\\nExample 3:\\nDocument: The Canadian Armed Forces. 1  The first large-scale Canadian peacekeeping mission started in Egypt on November 24, 1956. 2  There are approximately 65,000 Regular Force and 25,000 reservist members in the Canadian military. 3  In Canada, August 9 is designated as National Peacekeepers\\' Day.\\nRelevant Query: How large is the canadian military?\\n\\nExample 4:\\nDocument: The Delaware River flows through Philadelphia into the Delaware Bay. It flows through and aqueduct in the Roundout Reservoir and then flows through Philadelphia and New Jersey before emptying into the Delaware Bay.\\nRelevant Query: What is the Delaware River?\\n\\nExample 5:\\nDocument: We were invited to a party, so we invited our neighbors too. We\\'re having a barbecue with all the neighbors.\\nRelevant Query:         What are they having a barbecue for?\\n\\nExample 6:\\nDocument: Our favorite foods are eggs, bread, cheese, milk, and cheese. We like pasta.\\nRelevant Query: What types of food do you like?\\n\\nExample 7:\\nDocument: What do you drink?  I like coffee, tea, milk, and water.\\nRelevant Query: What do you like to drink?\\n\\nA:\\n\\nThere\\'s something a bit odd about \"Example 1\" and \"Example 2\" in particular. I suspect this could help in terms of the search-engines helping in a way that actually helps the searcher (which is something that the StackExchange engine doesn\\'t quite get at the moment). But that\\'s just a guess.\\n\\n<|endoftext|>']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_text[0][-1000:]"
      ],
      "metadata": {
        "id": "upsMonYyNiUD",
        "outputId": "da08bd50-da2c-4e58-9748-77c6c5618f76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Philadelphia into the Delaware Bay. It flows through and aqueduct in the Roundout Reservoir and then flows through Philadelphia and New Jersey before emptying into the Delaware Bay.\\nRelevant Query: What is the Delaware River?\\n\\nExample 5:\\nDocument: We were invited to a party, so we invited our neighbors too. We\\'re having a barbecue with all the neighbors.\\nRelevant Query:         What are they having a barbecue for?\\n\\nExample 6:\\nDocument: Our favorite foods are eggs, bread, cheese, milk, and cheese. We like pasta.\\nRelevant Query: What types of food do you like?\\n\\nExample 7:\\nDocument: What do you drink?  I like coffee, tea, milk, and water.\\nRelevant Query: What do you like to drink?\\n\\nA:\\n\\nThere\\'s something a bit odd about \"Example 1\" and \"Example 2\" in particular. I suspect this could help in terms of the search-engines helping in a way that actually helps the searcher (which is something that the StackExchange engine doesn\\'t quite get at the moment). But that\\'s just a guess.\\n\\n<|endoftext|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LAsE9FXLQ1II"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Exemplo de uso do GPT-J -GPT-3 open source ",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}