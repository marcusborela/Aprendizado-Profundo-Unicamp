{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classificandoImagensdeSatelite.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcusborela/Aprendizado-Profundo-Unicamp/blob/main/classificandoImagensdeSatelite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensaio de classificação de imagens de satélite Sentinel 2\n",
        "\n",
        "Remis Balaniuk, PhD\n",
        "\n",
        "Nesse caderno abordaremos as principais etapas e tarefas necessárias para implementar e treinar um rede convolucional na tarefa de classificação de imagens de satélite multi espectrais.\n"
      ],
      "metadata": {
        "id": "MHeIcJMsA5s3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqo4fDgNeEpX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f162f66-6274-4793-cbd5-580f4625f786"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "!pip install rasterio\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "!pip install simplekml\n",
        "import simplekml\n",
        "import psutil\n",
        "import humanize\n",
        "import GPUtil as GPU\n",
        "import rasterio\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import gc\n",
        "import glob\n",
        "import pickle as pi\n",
        "import re\n",
        "import math\n",
        "from random import shuffle\n",
        "import gdal\n",
        "import os\n",
        "from tempfile import NamedTemporaryFile\n",
        "import h5py\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,f1_score\n",
        "from google.colab import drive\n",
        "from IPython.display import IFrame\n",
        "\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "rseed = 100\n",
        "np.random.seed(rseed)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rasterio\n",
            "  Downloading rasterio-1.2.10-cp37-cp37m-manylinux1_x86_64.whl (19.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.3 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting affine\n",
            "  Downloading affine-2.3.1-py2.py3-none-any.whl (16 kB)\n",
            "Collecting snuggs>=1.4.1\n",
            "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.7/dist-packages (from rasterio) (7.1.2)\n",
            "Collecting cligj>=0.5\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Collecting click-plugins\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from rasterio) (2022.5.18.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rasterio) (1.21.6)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from rasterio) (21.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rasterio) (57.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.7/dist-packages (from snuggs>=1.4.1->rasterio) (3.0.9)\n",
            "Installing collected packages: snuggs, cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.3.1 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.2.10 snuggs-1.4.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gputil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=3d9469b02592d8ea56e2c34ecd620a11d45e4b10dfd2b153c663ff695bdaea85\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting simplekml\n",
            "  Downloading simplekml-1.3.6.tar.gz (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: simplekml\n",
            "  Building wheel for simplekml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for simplekml: filename=simplekml-1.3.6-py3-none-any.whl size=65876 sha256=c75dd09ebba219651b608880812fe8837d57588097cb58cdedae007edc47e9b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/ec/e6/10af1a1fb29ffca95151d4c886d6e06fc309c68f46519892de\n",
            "Successfully built simplekml\n",
            "Installing collected packages: simplekml\n",
            "Successfully installed simplekml-1.3.6\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Organização dos dados\n",
        "\n",
        "Vamos assumir que as imagens estão separadas em subdiretórios, um para cada classe.\n",
        "\n",
        "Posicione-se no seu diretório de diretórios de imagens.\n",
        "\n"
      ],
      "metadata": {
        "id": "wyZgFYGnKipN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Um9lb7mNL1qX",
        "outputId": "4246a686-e690-4b3b-8af2-6c77eeabd0fb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/DataForTailingDamDetection/Data/SentinelMinesDams/train/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLWByLfRL3U8",
        "outputId": "c7a20be7-7fab-4714-81ce-43faf14be11f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1aIK9Gf7J0xpdioKfUv6YsB1Br4r2vNYp/DataForTailingDamDetection/Data/SentinelMinesDams/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vDOmos4NyMX",
        "outputId": "0bb46b48-8d81-469c-883b-0978cb5d5dcd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mmine\u001b[0m/  \u001b[01;34mnotmine\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_labels = []\n",
        "for label_num, subdir in enumerate(next(os.walk('./'))[1]):\n",
        "    char_labels.append(subdir)\n"
      ],
      "metadata": {
        "id": "IDCTOyOlNczb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fi2Ue7uN5aI",
        "outputId": "2bbc4294-b3dd-4bcf-fefb-b676e60abaa8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mine', 'notmine']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_imagens = 0\n",
        "\n",
        "for subdir in char_labels:\n",
        "  n_imagens += len(os.listdir(subdir))"
      ],
      "metadata": {
        "id": "QTVvmWzKLV3N"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_imagens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx6brDf0MI8r",
        "outputId": "071e79b5-a59f-4e89-ffda-0b35a1ce1feb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2860"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definições iniciais:\n",
        "\n",
        "As imagens podem ter pequenas variações em número de pixels. Vamos estabelecer um tamanho padrão e adequar as imagens.\n",
        "\n",
        "Sentinel-2 Bands\tCentral Wavelength (µm)\tResolution (m)\n",
        "- Band 1 - Coastal aerosol\t0.443\t60\n",
        "- Band 2 - Blue\t0.490\t10\n",
        "- Band 3 - Green\t0.560\t10\n",
        "- Band 4 - Red\t0.665\t10\n",
        "- Band 5 - Vegetation Red Edge\t0.705\t20\n",
        "- Band 6 - Vegetation Red Edge\t0.740\t20\n",
        "- Band 7 - Vegetation Red Edge\t0.783\t20\n",
        "- Band 8 - NIR\t0.842\t10\n",
        "- Band 8A - Vegetation Red Edge\t0.865\t20\n",
        "- Band 9 - Water vapour\t0.945\t60\n",
        "- Band 10 - SWIR - Cirrus\t1.375\t60\n",
        "- Band 11 - SWIR\t1.610\t20\n",
        "- Band 12 - SWIR\t2.190\t20\n"
      ],
      "metadata": {
        "id": "JH2qIR5mLeMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_width = 200\n",
        "image_height = 200\n",
        "\n",
        "Nbands = 7\n",
        "\n",
        "# este script assume que estamos usando imagens do Sentinel 2 com 12 spectral bands mas selecionaremos só 10 \n",
        "#opt_string = '-outsize {} {} -b 2 -b 3 -b 4 -b 5 -b 6 -b 7 -b 8 -b 8A -b 11 -b 12'.format(image_height, image_width)\n",
        "\n",
        "# este script assume que estamos usando imagens do Sentinel 2 com 12 spectral bands mas selecionaremos só 7 por limitações de memória \n",
        "opt_string = '-outsize {} {} -b 2 -b 3 -b 4 -b 5 -b 7 -b 8 -b 11'.format(image_height, image_width)\n",
        "\n",
        "\n",
        "# quantas classes?\n",
        "n_classes = 2"
      ],
      "metadata": {
        "id": "F2AbiMrCKhVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definindo os arrays para as imagens a priori para evitar variações de tamanho nos dados."
      ],
      "metadata": {
        "id": "3rzkMY0jP4Jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del imagens\n",
        "del labels"
      ],
      "metadata": {
        "id": "NVLXd3qtXaOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imagens = np.zeros((n_imagens, image_width, image_height, Nbands), dtype=np.float32)\n",
        "labels = -1 * np.ones((n_imagens,), dtype=np.int64)\n"
      ],
      "metadata": {
        "id": "0U_0vC0WMPrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregando as imagens\n",
        "\n",
        "Para carregar e pré processar as imagens geoespaciais precisaremos usar uma biblioteca especializada chamada GDAL - Geospatial Data Abstraction Library https://gdal.org/api/python.html.\n",
        "\n",
        "É possível que a memória da VM do Colab não suporte um volume muito grande de imagens. "
      ],
      "metadata": {
        "id": "6C1wP3S2CUZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_counter = 0\n",
        "for i in range(len(char_labels)):\n",
        "\n",
        "  subdir = char_labels[i]\n",
        "  print(\"Carregando imagens para classe\", subdir)\n",
        "\n",
        "  files = np.array(os.listdir(subdir))\n",
        "  # fazendo permutação para evitar vies\n",
        "  permutation = np.random.permutation(len(files))\n",
        "  files = files[permutation]\n",
        "  \n",
        "  for file in files:\n",
        "      img_dataset = gdal.Translate(NamedTemporaryFile(delete=False).name,\n",
        "                                    gdal.Open(subdir + '/' + file, gdal.GA_ReadOnly),\n",
        "                                    options=opt_string)\n",
        "\n",
        "      img_array = np.transpose(np.array(img_dataset.ReadAsArray(),dtype = 'float32'), axes=(1, 2, 0))\n",
        "\n",
        "      linhas = min(image_height,img_array.shape[0])\n",
        "      colunas = min(image_height,img_array.shape[1])\n",
        "\n",
        "      imagens[image_counter, :linhas, :colunas, :] = img_array[:linhas,:colunas,0:Nbands]\n",
        "      labels[image_counter] = i\n",
        "      image_counter += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j45kdMPGOL23",
        "outputId": "f4b79b47-8f45-46c9-823c-aba0f70a07bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando imagens para classe mine\n",
            "Carregando imagens para classe notmine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd  /content/drive/MyDrive/TCU/curso sensoriamento remoto/Sensoriamento Remoto e Deep Learning/material alunos/minas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WibtYAMVUadF",
        "outputId": "b1f0451a-d53a-4a6f-a990-d7fbc4152781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/TCU/curso sensoriamento remoto/Sensoriamento Remoto e Deep Learning/material alunos/minas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Salvando as imagens em formato numpy \n",
        "\n",
        "Caso a VM caia..."
      ],
      "metadata": {
        "id": "fHNjTGJhY5fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.savez('./imagens',\n",
        "            images=imagens,\n",
        "            labels=labels,\n",
        "            charlabels = char_labels)"
      ],
      "metadata": {
        "id": "y-bkGpV-Ygqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separando em conjuntos de treinamento e teste"
      ],
      "metadata": {
        "id": "One08x01TdjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defina aqui o percentual a ser usado no treinamento\n",
        "split_ratio = 0.8\n",
        "\n",
        "permutation = np.random.permutation(n_imagens)\n",
        "imagens = imagens[permutation]\n",
        "labels = labels[permutation]\n",
        "\n",
        "Ntrain = int(n_imagens*split_ratio)\n",
        "\n",
        "train_images = imagens[:Ntrain] \n",
        "train_labels = labels[:Ntrain] \n",
        "\n",
        "test_images = imagens[Ntrain:]\n",
        "test_labels = labels[Ntrain:]"
      ],
      "metadata": {
        "id": "e64tM-_uTdGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizando os dados\n"
      ],
      "metadata": {
        "id": "1j6gNcTWS7ZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nmean = np.zeros(Nbands)\n",
        "nmax = np.zeros(Nbands)\n",
        "\n",
        "# Normalizando pixels usando z-score\n",
        "for i in range(Nbands):\n",
        "  nmean[i] = np.mean(train_images[:,:,:,i])  # média \n",
        "  nmax[i] = np.std(train_images[:,:,:,i])    # desvio padrão\n",
        "  train_images[:,:,:,i]=train_images[:,:,:,i]-nmean[i]\n",
        "  train_images[:,:,:,i]=train_images[:,:,:,i]/nmax[i]\n",
        "  test_images[:,:,:,i]=test_images[:,:,:,i]-nmean[i]\n",
        "  test_images[:,:,:,i]=test_images[:,:,:,i]/nmax[i]\n",
        "\n"
      ],
      "metadata": {
        "id": "TW956BT8S6yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Salvando dados de treinamento pré processados para carga mais rápida.\n",
        "\n",
        "Salvareremos também a média e o desvio padrão estimados na base de treinamento para uso posterior no pré processamento da base de teste.\n",
        "\n",
        "Posicione-se num diretório no qual você possa salvar dados do experimento. Tipicamente você vai realizar diversos experimentos, com diferentes configurações e arquiteturas, por isso é interessante deixar pastas separadas para cada um."
      ],
      "metadata": {
        "id": "yzGEpqwNSdgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.savez('./trainingData',\n",
        "            images=train_images,\n",
        "            labels=train_labels,\n",
        "            charlabels = char_labels)\n",
        "\n",
        "np.savez('./testingData',\n",
        "            images=test_images,\n",
        "            labels=test_labels,\n",
        "            charlabels = char_labels)\n",
        "\n",
        "np.savez('./modelMetaData',\n",
        "            metadata =np.array([image_width,image_height,Nbands,n_classes,char_labels])) \n",
        "\n",
        "np.savez('./trainingDataAvgStd',\n",
        "            median=nmean,\n",
        "            std=nmax)"
      ],
      "metadata": {
        "id": "dhLA7VcdScSX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2380f9ee-e181-43f2-abfd-701b39e3450f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregando datasets pré-processados de arquivo\n",
        "\n",
        "Só execute se for recomeçar desse ponto."
      ],
      "metadata": {
        "id": "a0v0Y4cLB6Gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd  /content/drive/MyDrive/TCU/curso sensoriamento remoto/Sensoriamento Remoto e Deep Learning/material alunos/minas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWaj4qwTcY79",
        "outputId": "cf136c85-981e-4434-d069-c4256ee8c3e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/TCU/curso sensoriamento remoto/Sensoriamento Remoto e Deep Learning/material alunos/minas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "npzfile = np.load('./trainingData.npz')\n",
        "train_images =     npzfile['images']\n",
        "train_labels =     npzfile['labels']\n",
        "char_labels =      npzfile['charlabels']\n",
        "\n",
        "npzfile = np.load('./testingData.npz')\n",
        "test_images =     npzfile['images']\n",
        "test_labels =     npzfile['labels']\n",
        "char_labels =      npzfile['charlabels']\n",
        "\n",
        "n_images = train_images.shape[0] + test_images.shape[0]\n",
        "metadata = np.load('./modelMetaData.npz',allow_pickle=True)\n",
        "image_width,image_height,Nbands,n_classes,char_labels = metadata['metadata'] \n",
        "\n",
        "# os dados de treinamento foram salvos já normalizados\n",
        "npzfile = np.load('./trainingDataAvgStd.npz',)\n",
        "nmean = npzfile['median']\n",
        "nmax = npzfile['std']\n"
      ],
      "metadata": {
        "id": "4gNtAosTHEN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj8z_H2hYFFO",
        "outputId": "7ff344aa-bdea-4e28-a46a-b95b9353b73a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(572, 200, 200, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definindo a arquitetura da rede"
      ],
      "metadata": {
        "id": "iEGoCUhXIm6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_cross_entropy_with_logits(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred))\n",
        "\n",
        "def custom_sparse_categorical_accuracy(y_true,y_pred):\n",
        "  sh=tf.shape(y_pred)\n",
        "  y_pred = tf.reshape(y_pred, [sh[0]*sh[1],n_classes])   # adjust the last argument for the number of classes \n",
        "  y_true = tf.reshape(y_true, [sh[0]*sh[1],1])\n",
        "  \n",
        "  return tf.keras.metrics.sparse_categorical_accuracy(y_true,y_pred)\n",
        "\n",
        "def custom_sparse_categorical_crossentropy(y_true,y_pred):\n",
        "  sh=tf.shape(y_pred)\n",
        "  y_pred = tf.reshape(y_pred, [sh[0]*sh[1],n_classes]) # adjust the last argument for the number of classes \n",
        "  y_true = tf.reshape(y_true, [sh[0]*sh[1],1])\n",
        " \n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(y_true,y_pred)\n"
      ],
      "metadata": {
        "id": "smKPv9foaWy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_FCN_discovery(image_width,image_height,Nbands):\n",
        "\n",
        "  #global n_classes\n",
        "  #n_classes = nc\n",
        "  kx = math.ceil((image_width-32)/27)  \n",
        "  ky = math.ceil((image_height-32)/27)  \n",
        "  # define the Fully Convolution Neural Network\n",
        "  input=layers.Input(shape=(None, None, Nbands))\n",
        "  x0=layers.Conv2D(32, (3, 3), (1,1), activation='relu', padding='SAME')(input)\n",
        "  x1=layers.MaxPooling2D((3, 3))(x0)\n",
        "  x2=layers.Conv2D(64, (3, 3), (1,1), activation='relu', padding='SAME')(x1)\n",
        "  x3=layers.MaxPooling2D((3, 3))(x2)\n",
        "  x4=layers.Conv2D(64, (3, 3), (1,1), activation='relu', padding='SAME')(x3)\n",
        "  x5=layers.MaxPooling2D((3,3))(x4)\n",
        "  x6=layers.Conv2D(64, (kx,ky), (1,1), activation = 'relu')(x5)  # the convolution kernel size must be adapted for image size \n",
        "  x7=layers.Dropout(0.5)(x6)\n",
        "  x8=layers.Conv2D(n_classes, (1,1), (1,1), padding='VALID', activation='softmax')(x7)\n",
        "  model=models.Model(input,x8)\n",
        "  model.summary()\n",
        "\n",
        "  model.compile(optimizer='adam',\n",
        "              loss=custom_sparse_categorical_crossentropy,\n",
        "              metrics=[custom_sparse_categorical_accuracy])\n",
        "             \n",
        "  return model\n"
      ],
      "metadata": {
        "id": "cC6xlxRs8wa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = define_FCN_discovery(image_width,image_height,Nbands)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBKfqepS-6G6",
        "outputId": "d528351c-c92d-4ede-e494-25bb5f8ecfe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None, None, 7)]   0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, None, None, 32)    2048      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, None, None, 32)   0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, None, None, 64)    18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, None, None, 64)   0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, None, None, 64)    36928     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, None, None, 64)   0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, None, None, 64)    200768    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, None, 64)    0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, None, None, 2)     130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 258,370\n",
            "Trainable params: 258,370\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinando a CNN"
      ],
      "metadata": {
        "id": "lU_g67OGI3dW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando um callback  que salve os pesos do modelo\n",
        "  \n",
        "epochs = 20\n",
        "batchsize = 10 # cuidado com o tamanho para não derrubar a máquina\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath= './trained_model/weights.ckpt',\n",
        "                                                save_weights_only=True,\n",
        "                                                verbose=1)\n",
        "\n",
        "fit = model.fit(train_images, train_labels, epochs=epochs, batch_size=batchsize,callbacks=[cp_callback])\n",
        "\n",
        "accuracy = fit.history[list(fit.history.keys())[-1]]\n",
        "\n",
        "plt.plot(range(epochs), accuracy, 'r--', label='train')\n",
        "plt.title('Training accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bILCGr_iI5gO",
        "outputId": "0e798840-9fae-41d1-e392-9a6cb9114091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.4377 - custom_sparse_categorical_accuracy: 0.8247\n",
            "Epoch 1: saving model to ./trained_model/weights.ckpt\n",
            "229/229 [==============================] - 24s 24ms/step - loss: 0.4377 - custom_sparse_categorical_accuracy: 0.8247\n",
            "Epoch 2/20\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.2705 - custom_sparse_categorical_accuracy: 0.8903\n",
            "Epoch 2: saving model to ./trained_model/weights.ckpt\n",
            "229/229 [==============================] - 4s 18ms/step - loss: 0.2705 - custom_sparse_categorical_accuracy: 0.8903\n",
            "Epoch 3/20\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.2048 - custom_sparse_categorical_accuracy: 0.9209\n",
            "Epoch 3: saving model to ./trained_model/weights.ckpt\n",
            "229/229 [==============================] - 4s 15ms/step - loss: 0.2048 - custom_sparse_categorical_accuracy: 0.9209\n",
            "Epoch 4/20\n",
            "229/229 [==============================] - ETA: 0s - loss: 0.1874 - custom_sparse_categorical_accuracy: 0.9318\n",
            "Epoch 4: saving model to ./trained_model/weights.ckpt\n",
            "229/229 [==============================] - 4s 16ms/step - loss: 0.1874 - custom_sparse_categorical_accuracy: 0.9318\n",
            "Epoch 5/20\n",
            "227/229 [============================>.] - ETA: 0s - loss: 0.1838 - custom_sparse_categorical_accuracy: 0.9361\n",
            "Epoch 5: saving model to ./trained_model/weights.ckpt\n",
            "229/229 [==============================] - 3s 15ms/step - loss: 0.1857 - custom_sparse_categorical_accuracy: 0.9358\n",
            "Epoch 6/20\n",
            "227/229 [============================>.] - ETA: 0s - loss: 0.1671 - custom_sparse_categorical_accuracy: 0.9445\n",
            "Epoch 6: saving model to ./trained_model/weights.ckpt\n",
            "229/229 [==============================] - 3s 15ms/step - loss: 0.1659 - custom_sparse_categorical_accuracy: 0.9449\n",
            "Epoch 7/20\n",
            "227/229 [============================>.] - ETA: 0s - loss: 0.1128 - custom_sparse_categorical_accuracy: 0.9612\n",
            "Epoch 7: saving model to ./trained_model/weights.ckpt\n",
            "229/229 [==============================] - 3s 15ms/step - loss: 0.1127 - custom_sparse_categorical_accuracy: 0.9611\n",
            "Epoch 8/20\n",
            "228/229 [============================>.] - ETA: 0s - loss: 0.0972 - custom_sparse_categorical_accuracy: 0.9689\n",
            "Epoch 8: saving model to ./trained_model/weights.ckpt\n",
            "229/229 [==============================] - 3s 15ms/step - loss: 0.0970 - custom_sparse_categorical_accuracy: 0.9690\n",
            "Epoch 9/20\n",
            "228/229 [============================>.] - ETA: 0s - loss: 0.0950 - custom_sparse_categorical_accuracy: 0.9697\n",
            "Epoch 9: saving model to ./trained_model/weights.ckpt\n",
            "229/229 [==============================] - 4s 15ms/step - loss: 0.0950 - custom_sparse_categorical_accuracy: 0.9698\n",
            "Epoch 10/20\n",
            "226/229 [============================>.] - ETA: 0s - loss: 0.0812 - custom_sparse_categorical_accuracy: 0.9743\n",
            "Epoch 10: saving model to ./trained_model/weights.ckpt\n",
            "229/229 [==============================] - 4s 15ms/step - loss: 0.0810 - custom_sparse_categorical_accuracy: 0.9747\n",
            "Epoch 11/20\n",
            "226/229 [============================>.] - ETA: 0s - loss: 0.0795 - custom_sparse_categorical_accuracy: 0.9796\n",
            "Epoch 11: saving model to ./trained_model/weights.ckpt\n",
            "229/229 [==============================] - 3s 15ms/step - loss: 0.0789 - custom_sparse_categorical_accuracy: 0.9799\n",
            "Epoch 12/20\n",
            "227/229 [============================>.] - ETA: 0s - loss: 0.0522 - custom_sparse_categorical_accuracy: 0.9819\n",
            "Epoch 12: saving model to ./trained_model/weights.ckpt\n",
            "229/229 [==============================] - 3s 15ms/step - loss: 0.0519 - custom_sparse_categorical_accuracy: 0.9821\n",
            "Epoch 13/20\n",
            "227/229 [============================>.] - ETA: 0s - loss: 0.0325 - custom_sparse_categorical_accuracy: 0.9890\n",
            "Epoch 13: saving model to ./trained_model/weights.ckpt\n",
            "229/229 [==============================] - 4s 16ms/step - loss: 0.0346 - custom_sparse_categorical_accuracy: 0.9886\n",
            "Epoch 14/20\n",
            "228/229 [============================>.] - ETA: 0s - loss: 0.1004 - custom_sparse_categorical_accuracy: 0.9658\n",
            "Epoch 14: saving model to ./trained_model/weights.ckpt\n",
            "229/229 [==============================] - 4s 16ms/step - loss: 0.1000 - custom_sparse_categorical_accuracy: 0.9659\n",
            "Epoch 15/20\n",
            "226/229 [============================>.] - ETA: 0s - loss: 0.0321 - custom_sparse_categorical_accuracy: 0.9898\n",
            "Epoch 15: saving model to ./trained_model/weights.ckpt\n",
            "229/229 [==============================] - 3s 15ms/step - loss: 0.0326 - custom_sparse_categorical_accuracy: 0.9895\n",
            "Epoch 16/20\n",
            "226/229 [============================>.] - ETA: 0s - loss: 0.0276 - custom_sparse_categorical_accuracy: 0.9903\n",
            "Epoch 16: saving model to ./trained_model/weights.ckpt\n",
            "229/229 [==============================] - 4s 15ms/step - loss: 0.0273 - custom_sparse_categorical_accuracy: 0.9904\n",
            "Epoch 17/20\n",
            "227/229 [============================>.] - ETA: 0s - loss: 0.0368 - custom_sparse_categorical_accuracy: 0.9877\n",
            "Epoch 17: saving model to ./trained_model/weights.ckpt\n",
            "229/229 [==============================] - 3s 15ms/step - loss: 0.0367 - custom_sparse_categorical_accuracy: 0.9878\n",
            "Epoch 18/20\n",
            "228/229 [============================>.] - ETA: 0s - loss: 0.0524 - custom_sparse_categorical_accuracy: 0.9820\n",
            "Epoch 18: saving model to ./trained_model/weights.ckpt\n",
            "229/229 [==============================] - 4s 15ms/step - loss: 0.0522 - custom_sparse_categorical_accuracy: 0.9821\n",
            "Epoch 19/20\n",
            "228/229 [============================>.] - ETA: 0s - loss: 0.0460 - custom_sparse_categorical_accuracy: 0.9846\n",
            "Epoch 19: saving model to ./trained_model/weights.ckpt\n",
            "229/229 [==============================] - 4s 15ms/step - loss: 0.0458 - custom_sparse_categorical_accuracy: 0.9847\n",
            "Epoch 20/20\n",
            "227/229 [============================>.] - ETA: 0s - loss: 0.0198 - custom_sparse_categorical_accuracy: 0.9930\n",
            "Epoch 20: saving model to ./trained_model/weights.ckpt\n",
            "229/229 [==============================] - 4s 15ms/step - loss: 0.0197 - custom_sparse_categorical_accuracy: 0.9930\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregando um model salvo"
      ],
      "metadata": {
        "id": "TkvCdPezJl5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd  /content/drive/MyDrive/TCU/curso sensoriamento remoto/Sensoriamento Remoto e Deep Learning/material alunos/minas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-zEr2yh_zRF",
        "outputId": "4458f1f2-9af2-4771-eff7-387051aca77d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/TCU/curso sensoriamento remoto/Sensoriamento Remoto e Deep Learning/material alunos/minas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists('./trained_model/checkpoint'): \n",
        "  metadata = np.load('./modelMetaData.npz',allow_pickle=True)\n",
        "  image_width,image_height,Nbands,n_classes,char_labels = metadata['metadata'] \n",
        "\n",
        "  model = define_FCN_discovery(image_width,image_height,Nbands)\n",
        "  \n",
        "  model.load_weights('./trained_model/weights.ckpt')\n",
        "    \n",
        "  npzfile = np.load('./trainingDataAvgStd.npz',)\n",
        "  nmean = npzfile['median']\n",
        "  nmax = npzfile['std']"
      ],
      "metadata": {
        "id": "aCyIdWwrJo-U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c0216b-45d9-46a3-bbb3-8892f3e7a401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None, None, 7)]   0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, None, None, 32)    2048      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, None, None, 32)   0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, None, None, 64)    18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, None, None, 64)   0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, None, None, 64)    36928     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, None, None, 64)   0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, None, None, 64)    200768    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, None, 64)    0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, None, None, 2)     130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 258,370\n",
            "Trainable params: 258,370\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testando o modelo "
      ],
      "metadata": {
        "id": "fNMqMTAgJ9pK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict(train_images)\n",
        "\n",
        "predics = np.zeros(len(prediction))\n",
        "\n",
        "predics = prediction[:,0,0,:].argmax(axis=1) \n",
        "\n",
        "print(confusion_matrix(train_labels, predics))\n"
      ],
      "metadata": {
        "id": "Dfv0HjxLZ1Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "print(\"Testing:\")\n",
        "prediction = model.predict(test_images)\n",
        "\n",
        "predics = np.zeros(len(prediction))\n",
        "\n",
        "predics = prediction[:,0,0,:].argmax(axis=1) \n",
        "\n",
        "print(confusion_matrix(test_labels, predics))\n",
        "\n",
        "y_true = test_labels\n",
        "y_pred = predics\n",
        "\n",
        "if n_classes==2:\n",
        "  print(\"Precision: \", metrics.precision_score(y_true, y_pred))\n",
        "  print(\"Recall: \", metrics.recall_score(y_true, y_pred))\n",
        "  print(\"F1 score: \", metrics.f1_score(y_true, y_pred))\n",
        "  print(\"Mean accuracy: \", metrics.accuracy_score(y_true, y_pred))\n",
        "  print(\"AUC: \", metrics.roc_auc_score(y_true, y_pred))\n",
        "  print(\"mcc: \", metrics.matthews_corrcoef(y_true, y_pred))\n",
        "  print(\"kappa: \", metrics.cohen_kappa_score(y_true, y_pred))\n"
      ],
      "metadata": {
        "id": "pYymY3CpJ7mt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9d57f85-a879-44f5-a30d-3bf628860618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing:\n",
            "[[268   5]\n",
            " [  1 298]]\n",
            "Precision:  0.9834983498349835\n",
            "Recall:  0.9966555183946488\n",
            "F1 score:  0.9900332225913622\n",
            "Mean accuracy:  0.9895104895104895\n",
            "AUC:  0.9891702500398153\n",
            "mcc:  0.9790604240153693\n",
            "kappa:  0.9789641434262948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgXd8fGorPxz"
      },
      "source": [
        "# Aplicando o modelo em grandes imagens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(modelo,labels,experiment_name,image_width,image_height,nmean,nmax,Nbands,path_to_predict_images,path_to_results,slicing = 1):\n",
        "\n",
        "  countImages = 0\n",
        "  \n",
        "  print(\"Escolha a classe alvo: \")\n",
        "  for i in range(0, len(labels)):\n",
        "    print(i,\":\",labels[i])\n",
        "  try:\n",
        "    save=int(input('Input:'))\n",
        "  except ValueError:\n",
        "    print(\"Not a number\")\n",
        "\n",
        "  kmlFull=simplekml.Kml()\n",
        "  resultsFull = [] \n",
        "\n",
        "  # processa cada imagem no diretório de predição\n",
        "  for file in os.listdir(path_to_predict_images):\n",
        "    print(path_to_predict_images + file)\n",
        "\n",
        "    starthere = len(resultsFull)\n",
        "\n",
        "    testimage = gdal.Translate(NamedTemporaryFile(delete=False).name,\n",
        "                                  gdal.Open(path_to_predict_images + file, gdal.GA_ReadOnly),\n",
        "                                  options = '-b 2 -b 3 -b 4 -b 5 -b 7 -b 8 -b 11')           # use as mesmas bandas\n",
        "\n",
        "    print(\"imagem carregada\")\n",
        "    testimage = np.transpose(np.array(testimage.ReadAsArray(),dtype = 'float32'), axes=(1, 2, 0))\n",
        "    testimage = testimage[:,:,0:Nbands]\n",
        "    for i in range(Nbands):\n",
        "      testimage[:,:,i]=testimage[:,:,i]-nmean[i]\n",
        "      testimage[:,:,i]=testimage[:,:,i]/nmax[i]\n",
        "\n",
        "    # corta a imagem para previsão se for muito grande para o colab. \n",
        "    # divide a imagem em patches regulares 2D \n",
        "    # Adapte o \"slicing\" ao seu tamanho de imagem e disponibilidade de memória no colab  \n",
        "    \n",
        "    sh = np.array(testimage.shape)\n",
        "    sh[0] = sh[0]//slicing\n",
        "    sh[1] = sh[1]//slicing\n",
        "    prediction = None\n",
        "\n",
        "    print(\"imagem processada\")\n",
        "\n",
        "    for i in range(0,slicing):  \n",
        "      for j in range(0,slicing):\n",
        "        patch = np.expand_dims(testimage[sh[0]*i:sh[0]*(i+1),sh[1]*j:sh[1]*(j+1),:],axis=0) \n",
        "        # compute prediction of the FCN\n",
        "        if prediction is None:\n",
        "          prediction = model.predict(patch)\n",
        "        else:\n",
        "          prediction = np.append(prediction,model.predict(patch), axis=0)\n",
        "      \n",
        "    # locate the predictions on the patches\n",
        "\n",
        "    shp = prediction.shape\n",
        "    results = []\n",
        "    lpatches = []\n",
        "    kml=simplekml.Kml()\n",
        "    count_positives = 0\n",
        "\n",
        "    with rasterio.open(path_to_predict_images+file) as map_layer:\n",
        "      for i in range(0,slicing):  \n",
        "        for j in range(0,slicing): \n",
        " \n",
        "          print('slice:',i,j)\n",
        "          pos = i*slicing+j\n",
        "          \n",
        "          pred_classes = tf.greater(prediction[pos,:,:,save],0.6) # considerando um limite de probabilidade de 60% para aceitar um resultado\n",
        "          shpc = tf.shape(pred_classes)\n",
        "\n",
        "          inds = tf.where(pred_classes==True)\n",
        "\n",
        "          # recalculando a posição para a dimensão original da imagem\n",
        "          \n",
        "          xloc = i*sh[0]+np.array(tf.gather(np.linspace(image_width/2.0,sh[0]-image_width/2.0,num=shpc[0]),inds[:,0]))\n",
        "          yloc = j*sh[1]+np.array(tf.gather(np.linspace(image_height/2.0,sh[1]-image_height/2.0,num=shpc[1]),inds[:,1]))\n",
        "          predloc = prediction[pos,inds[:,0],inds[:,1],save]\n",
        "\n",
        "          for k in range(0,len(xloc)):\n",
        "            pixels2coords = map_layer.xy(xloc[k] , yloc[k]) \n",
        "\n",
        "            r = np.ones(4)*-1      \n",
        "            \n",
        "            r[0] = pixels2coords[0]\n",
        "            r[1] = pixels2coords[1]\n",
        "            r[2] = predloc[k]\n",
        "\n",
        "            tag = labels[save]\n",
        "              \n",
        "            kml.newpoint(name=tag, coords=[(pixels2coords[0],pixels2coords[1])])\n",
        "            kmlFull.newpoint(name=tag, coords=[(pixels2coords[0],pixels2coords[1])])\n",
        "\n",
        "            results.append(r)\n",
        "            resultsFull.append(r)\n",
        "\n",
        "    # save prediction files to the results folder\n",
        "    np.savetxt(path_to_results + 'prediction_{}.csv'.format(file), results, delimiter=\",\")\n",
        "    kml.save(path_to_results + 'predictions_{}.kml'.format(file))  \n",
        "\n",
        "    countImages+=1\n",
        "\n",
        "    del testimage\n",
        "    del patch\n",
        "    del prediction\n",
        "    del results\n",
        "    del kml\n",
        "    gc.collect()\n",
        "\n",
        "  np.savetxt(path_to_results + 'prediction_{}.csv'.format(experiment_name), np.array(resultsFull), delimiter=\",\")\n",
        "  kmlFull.save(path_to_results + 'predictions_{}.kml'.format(experiment_name))  \n"
      ],
      "metadata": {
        "id": "T4BrW-WRbQ6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ajuste os diretórios"
      ],
      "metadata": {
        "id": "hinQYPLbAPYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_results =   '/content/drive/MyDrive/TCU/curso sensoriamento remoto/Sensoriamento Remoto e Deep Learning/material alunos/minas/'\n",
        "path_to_predict_images = '/content/drive/MyDrive/DataForTailingDamDetection/Data/SentinelMinesDams/predict/'\n",
        "\n",
        "predict(model,char_labels,'minas',image_width,image_height,nmean,nmax,Nbands,path_to_predict_images,path_to_results,slicing=10)"
      ],
      "metadata": {
        "id": "qla53ZZg_EM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROODv-k5gnr8"
      },
      "source": [
        "## A previsão gera arquivos csv e kml com coordenadas dos pontos encontrados"
      ]
    }
  ]
}