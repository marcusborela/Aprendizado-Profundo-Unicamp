{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classificandoImagensdeSatelite.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcusborela/Aprendizado-Profundo-Unicamp/blob/main/classificandoImagensdeSatelite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensaio de classificação de imagens de satélite Sentinel 2\n",
        "\n",
        "Remis Balaniuk, PhD\n",
        "\n",
        "Nesse caderno abordaremos as principais etapas e tarefas necessárias para implementar e treinar um rede convolucional na tarefa de classificação de imagens de satélite multi espectrais.\n"
      ],
      "metadata": {
        "id": "MHeIcJMsA5s3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqo4fDgNeEpX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d64ff61-5631-4581-d08a-a282ffe5a3f0"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "!pip install rasterio\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "!pip install simplekml\n",
        "import simplekml\n",
        "import psutil\n",
        "import humanize\n",
        "import GPUtil as GPU\n",
        "import rasterio\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import gc\n",
        "import glob\n",
        "import pickle as pi\n",
        "import re\n",
        "import math\n",
        "from random import shuffle\n",
        "import gdal\n",
        "import os\n",
        "from tempfile import NamedTemporaryFile\n",
        "import h5py\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,f1_score\n",
        "from google.colab import drive\n",
        "from IPython.display import IFrame\n",
        "\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "rseed = 100\n",
        "np.random.seed(rseed)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rasterio\n",
            "  Using cached rasterio-1.2.10-cp37-cp37m-manylinux1_x86_64.whl (19.3 MB)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.7/dist-packages (from rasterio) (0.7.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from rasterio) (21.4.0)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.7/dist-packages (from rasterio) (1.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rasterio) (57.4.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from rasterio) (2022.5.18.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rasterio) (1.21.6)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.7/dist-packages (from rasterio) (2.3.1)\n",
            "Requirement already satisfied: snuggs>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from rasterio) (1.4.7)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.7/dist-packages (from rasterio) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.7/dist-packages (from snuggs>=1.4.1->rasterio) (3.0.9)\n",
            "Installing collected packages: rasterio\n",
            "Successfully installed rasterio-1.2.10\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gputil in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting simplekml\n",
            "  Using cached simplekml-1.3.6-py3-none-any.whl\n",
            "Installing collected packages: simplekml\n",
            "Successfully installed simplekml-1.3.6\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Organização dos dados\n",
        "\n",
        "Vamos assumir que as imagens estão separadas em subdiretórios, um para cada classe.\n",
        "\n",
        "Posicione-se no seu diretório de diretórios de imagens.\n",
        "\n"
      ],
      "metadata": {
        "id": "wyZgFYGnKipN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Um9lb7mNL1qX",
        "outputId": "0001e12e-6b37-453c-fe36-32dae197419d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/DataForTailingDamDetection/Data/SentinelMinesDams/train/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLWByLfRL3U8",
        "outputId": "dcbbcd63-50cc-4645-def5-fc73a39fa83f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1aIK9Gf7J0xpdioKfUv6YsB1Br4r2vNYp/DataForTailingDamDetection/Data/SentinelMinesDams/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vDOmos4NyMX",
        "outputId": "73bfad0d-9462-4a72-ef75-49f41ec8fc33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mmine\u001b[0m/              \u001b[01;34mnotmine\u001b[0m/         trainingDataAvgStd.npz\n",
            "modelMetaData.npz  testingData.npz  trainingData.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_labels = []\n",
        "for label_num, subdir in enumerate(next(os.walk('./'))[1]):\n",
        "    char_labels.append(subdir)\n"
      ],
      "metadata": {
        "id": "IDCTOyOlNczb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fi2Ue7uN5aI",
        "outputId": "e3d7a98e-4efa-410e-aba9-305cd97f54b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mine', 'notmine']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_imagens = 0\n",
        "\n",
        "for subdir in char_labels:\n",
        "  n_imagens += len(os.listdir(subdir))"
      ],
      "metadata": {
        "id": "QTVvmWzKLV3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_imagens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx6brDf0MI8r",
        "outputId": "782bf767-2994-4d3f-8e7a-a50d85991274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2860"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definições iniciais:\n",
        "\n",
        "As imagens podem ter pequenas variações em número de pixels. Vamos estabelecer um tamanho padrão e adequar as imagens.\n",
        "\n",
        "Sentinel-2 Bands\tCentral Wavelength (µm)\tResolution (m)\n",
        "- Band 1 - Coastal aerosol\t0.443\t60 umidade\n",
        "- Band 2 - Blue\t0.490\t10\n",
        "- Band 3 - Green\t0.560\t10\n",
        "- Band 4 - Red\t0.665\t10\n",
        "- Band 5 - Vegetation Red Edge\t0.705\t20  \n",
        "- Band 6 - Vegetation Red Edge\t0.740\t20\n",
        "- Band 7 - Vegetation Red Edge\t0.783\t20\n",
        "- Band 8 - NIR\t0.842\t10 infravermelho próximo (NEXT)\n",
        "- Band 8A - Vegetation Red Edge\t0.865\t20\n",
        "- Band 9 - Water vapour\t0.945\t60\n",
        "- Band 10 - SWIR - Cirrus\t1.375\t60\n",
        "- Band 11 - SWIR\t1.610\t20  Infra distante\n",
        "- Band 12 - SWIR\t2.190\t20  Infra distante"
      ],
      "metadata": {
        "id": "JH2qIR5mLeMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_width = 200\n",
        "image_height = 200\n",
        "\n",
        "Nbands = 7\n",
        "\n",
        "# este script assume que estamos usando imagens do Sentinel 2 com 12 spectral bands mas selecionaremos só 10 \n",
        "#opt_string = '-outsize {} {} -b 2 -b 3 -b 4 -b 5 -b 6 -b 7 -b 8 -b 8A -b 11 -b 12'.format(image_height, image_width)\n",
        "\n",
        "# qdo extraiu do google earth, pediu pra pixel ser de 10mts\n",
        "# mesmo bandas com pixels maiores foram subdivididos: copia e cola\n",
        "# este script assume que estamos usando imagens do Sentinel 2 com 12 spectral bands mas selecionaremos só 7 por limitações de memória \n",
        "opt_string = '-outsize {} {} -b 2 -b 3 -b 4 -b 5 -b 7 -b 8 -b 11'.format(image_height, image_width)\n",
        "\n",
        "\n",
        "# quantas classes?\n",
        "n_classes = 2"
      ],
      "metadata": {
        "id": "F2AbiMrCKhVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definindo os arrays para as imagens a priori para evitar variações de tamanho nos dados."
      ],
      "metadata": {
        "id": "3rzkMY0jP4Jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# del imagens\n",
        "# del labels"
      ],
      "metadata": {
        "id": "NVLXd3qtXaOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imagens = np.zeros((n_imagens, image_width, image_height, Nbands), dtype=np.float32)\n",
        "labels = -1 * np.ones((n_imagens,), dtype=np.int64)\n"
      ],
      "metadata": {
        "id": "0U_0vC0WMPrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregando as imagens\n",
        "\n",
        "Para carregar e pré processar as imagens geoespaciais precisaremos usar uma biblioteca especializada chamada GDAL - Geospatial Data Abstraction Library https://gdal.org/api/python.html.\n",
        "\n",
        "É possível que a memória da VM do Colab não suporte um volume muito grande de imagens. "
      ],
      "metadata": {
        "id": "6C1wP3S2CUZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_counter = 0\n",
        "for i in range(len(char_labels)):\n",
        "\n",
        "  subdir = char_labels[i]\n",
        "  print(\"Carregando imagens para classe\", subdir)\n",
        "\n",
        "  files = np.array(os.listdir(subdir))\n",
        "  # fazendo permutação para evitar vies\n",
        "  permutation = np.random.permutation(len(files))\n",
        "  files = files[permutation]\n",
        "  \n",
        "  for file in files:\n",
        "      img_dataset = gdal.Translate(NamedTemporaryFile(delete=False).name,\n",
        "                                    gdal.Open(subdir + '/' + file, gdal.GA_ReadOnly),\n",
        "                                    options=opt_string)\n",
        "\n",
        "      # queremos 200x200 e os canais um atrás do outro\n",
        "      img_array = np.transpose(np.array(img_dataset.ReadAsArray(),dtype = 'float32'), axes=(1, 2, 0))\n",
        "\n",
        "      linhas = min(image_height,img_array.shape[0])\n",
        "      colunas = min(image_height,img_array.shape[1])\n",
        "\n",
        "      imagens[image_counter, :linhas, :colunas, :] = img_array[:linhas,:colunas,0:Nbands]\n",
        "      labels[image_counter] = i\n",
        "      image_counter += 1\n"
      ],
      "metadata": {
        "id": "j45kdMPGOL23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Salvando as imagens em formato numpy \n",
        "\n",
        "Caso a VM caia..."
      ],
      "metadata": {
        "id": "fHNjTGJhY5fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bQldXRStWyeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd  /content/drive/MyDrive/curso_analise_imagens_satelite/minas"
      ],
      "metadata": {
        "id": "WibtYAMVUadF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "id": "VuYMOI6xQo9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.savez('./imagens',\n",
        "            images=imagens,\n",
        "            labels=labels,\n",
        "            charlabels = char_labels)"
      ],
      "metadata": {
        "id": "y-bkGpV-Ygqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separando em conjuntos de treinamento e teste"
      ],
      "metadata": {
        "id": "One08x01TdjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defina aqui o percentual a ser usado no treinamento\n",
        "split_ratio = 0.8\n",
        "\n",
        "permutation = np.random.permutation(n_imagens)\n",
        "imagens = imagens[permutation]\n",
        "labels = labels[permutation]\n",
        "\n",
        "Ntrain = int(n_imagens*split_ratio)\n",
        "\n",
        "train_images = imagens[:Ntrain] \n",
        "train_labels = labels[:Ntrain] \n",
        "\n",
        "test_images = imagens[Ntrain:]\n",
        "test_labels = labels[Ntrain:]"
      ],
      "metadata": {
        "id": "e64tM-_uTdGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizando os dados\n"
      ],
      "metadata": {
        "id": "1j6gNcTWS7ZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nmean = np.zeros(Nbands)\n",
        "nmax = np.zeros(Nbands)\n",
        "\n",
        "# Normalizando pixels usando z-score\n",
        "for i in range(Nbands):\n",
        "  nmean[i] = np.mean(train_images[:,:,:,i])  # média \n",
        "  nmax[i] = np.std(train_images[:,:,:,i])    # desvio padrão\n",
        "  train_images[:,:,:,i]=train_images[:,:,:,i]-nmean[i]\n",
        "  train_images[:,:,:,i]=train_images[:,:,:,i]/nmax[i]\n",
        "  test_images[:,:,:,i]=test_images[:,:,:,i]-nmean[i]\n",
        "  test_images[:,:,:,i]=test_images[:,:,:,i]/nmax[i]\n",
        "\n"
      ],
      "metadata": {
        "id": "TW956BT8S6yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Salvando dados de treinamento pré processados para carga mais rápida.\n",
        "\n",
        "Salvareremos também a média e o desvio padrão estimados na base de treinamento para uso posterior no pré processamento da base de teste.\n",
        "\n",
        "Posicione-se num diretório no qual você possa salvar dados do experimento. Tipicamente você vai realizar diversos experimentos, com diferentes configurações e arquiteturas, por isso é interessante deixar pastas separadas para cada um."
      ],
      "metadata": {
        "id": "yzGEpqwNSdgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.savez('./trainingData',\n",
        "            images=train_images,\n",
        "            labels=train_labels,\n",
        "            charlabels = char_labels)\n",
        "\n",
        "np.savez('./testingData',\n",
        "            images=test_images,\n",
        "            labels=test_labels,\n",
        "            charlabels = char_labels)\n",
        "\n",
        "np.savez('./modelMetaData',\n",
        "            metadata =np.array([image_width,image_height,Nbands,n_classes,char_labels])) \n",
        "\n",
        "np.savez('./trainingDataAvgStd',\n",
        "            median=nmean,\n",
        "            std=nmax)"
      ],
      "metadata": {
        "id": "dhLA7VcdScSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregando datasets pré-processados de arquivo\n",
        "\n",
        "Só execute se for recomeçar desse ponto."
      ],
      "metadata": {
        "id": "a0v0Y4cLB6Gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/drive/MyDrive/curso_analise_imagens_satelite/minas'"
      ],
      "metadata": {
        "id": "boegWSeka58c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/curso_analise_imagens_satelite/minas'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWaj4qwTcY79",
        "outputId": "1cb79eda-4594-433d-a054-852e83ee2d7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/curso_analise_imagens_satelite/minas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "npzfile = np.load('./trainingData.npz')\n",
        "train_images =     npzfile['images']\n",
        "train_labels =     npzfile['labels']\n",
        "char_labels =      npzfile['charlabels']\n",
        "\n",
        "npzfile = np.load('./testingData.npz')\n",
        "test_images =     npzfile['images']\n",
        "test_labels =     npzfile['labels']\n",
        "char_labels =      npzfile['charlabels']\n",
        "\n",
        "n_images = train_images.shape[0] + test_images.shape[0]\n",
        "metadata = np.load('./modelMetaData.npz',allow_pickle=True)\n",
        "image_width,image_height,Nbands,n_classes,char_labels = metadata['metadata'] \n",
        "\n",
        "# os dados de treinamento foram salvos já normalizados\n",
        "npzfile = np.load('./trainingDataAvgStd.npz',)\n",
        "nmean = npzfile['median']\n",
        "nmax = npzfile['std']\n"
      ],
      "metadata": {
        "id": "4gNtAosTHEN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj8z_H2hYFFO",
        "outputId": "c3bbf0c1-de4c-4892-ab1d-f07932d620d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(572, 200, 200, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definindo a arquitetura da rede"
      ],
      "metadata": {
        "id": "iEGoCUhXIm6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_cross_entropy_with_logits(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred))\n",
        "\n",
        "def custom_sparse_categorical_accuracy(y_true,y_pred):\n",
        "  sh=tf.shape(y_pred)\n",
        "  y_pred = tf.reshape(y_pred, [sh[0]*sh[1],n_classes])   # adjust the last argument for the number of classes \n",
        "  y_true = tf.reshape(y_true, [sh[0]*sh[1],1])\n",
        "  \n",
        "  return tf.keras.metrics.sparse_categorical_accuracy(y_true,y_pred)\n",
        "\n",
        "def custom_sparse_categorical_crossentropy(y_true,y_pred):\n",
        "  sh=tf.shape(y_pred)\n",
        "  y_pred = tf.reshape(y_pred, [sh[0]*sh[1],n_classes]) # adjust the last argument for the number of classes \n",
        "  y_true = tf.reshape(y_true, [sh[0]*sh[1],1])\n",
        " \n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(y_true,y_pred)\n"
      ],
      "metadata": {
        "id": "smKPv9foaWy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_FCN_discovery(image_width,image_height,Nbands):\n",
        "\n",
        "  #global n_classes\n",
        "  #n_classes = nc\n",
        "  kx = math.ceil((image_width-32)/27)  \n",
        "  ky = math.ceil((image_height-32)/27)  \n",
        "  # define the Fully Convolution Neural Network\n",
        "  input=layers.Input(shape=(None, None, Nbands))\n",
        "  x0=layers.Conv2D(32, (3, 3), (1,1), activation='relu', padding='SAME')(input)\n",
        "  x1=layers.MaxPooling2D((3, 3))(x0)\n",
        "  x2=layers.Conv2D(64, (3, 3), (1,1), activation='relu', padding='SAME')(x1)\n",
        "  x3=layers.MaxPooling2D((3, 3))(x2)\n",
        "  x4=layers.Conv2D(64, (3, 3), (1,1), activation='relu', padding='SAME')(x3)\n",
        "  x5=layers.MaxPooling2D((3,3))(x4)\n",
        "  x6=layers.Conv2D(64, (kx,ky), (1,1), activation = 'relu')(x5)  # the convolution kernel size must be adapted for image size \n",
        "  x7=layers.Dropout(0.5)(x6)\n",
        "  x8=layers.Conv2D(n_classes, (1,1), (1,1), padding='VALID', activation='softmax')(x7)\n",
        "  model=models.Model(input,x8)\n",
        "  model.summary()\n",
        "\n",
        "  model.compile(optimizer='adam',\n",
        "              loss=custom_sparse_categorical_crossentropy,\n",
        "              metrics=[custom_sparse_categorical_accuracy])\n",
        "             \n",
        "  return model\n",
        "\n",
        "  # rede fica puramente convolucional, menor\n",
        "  # sem flatten e sem  fully connected\n"
      ],
      "metadata": {
        "id": "cC6xlxRs8wa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = define_FCN_discovery(image_width,image_height,Nbands)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBKfqepS-6G6",
        "outputId": "3b388d18-3475-402b-83cd-b289cc293096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None, None, 7)]   0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, None, None, 32)    2048      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, None, None, 32)   0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, None, None, 64)    18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, None, None, 64)   0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, None, None, 64)    36928     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, None, None, 64)   0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, None, None, 64)    200768    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, None, 64)    0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, None, None, 2)     130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 258,370\n",
            "Trainable params: 258,370\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinando a CNN"
      ],
      "metadata": {
        "id": "lU_g67OGI3dW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando um callback  que salve os pesos do modelo\n",
        "  \n",
        "epochs = 20\n",
        "batchsize = 5 # cuidado com o tamanho para não derrubar a máquina\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath= './trained_model/weights.ckpt',\n",
        "                                                save_weights_only=True,\n",
        "                                                verbose=1)\n"
      ],
      "metadata": {
        "id": "bILCGr_iI5gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fit = model.fit(train_images, train_labels, epochs=epochs, batch_size=batchsize,callbacks=[cp_callback])\n",
        "\n",
        "accuracy = fit.history[list(fit.history.keys())[-1]]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEs2TsDBWIT4",
        "outputId": "6338c6c0-bd09-45b2-f845-9e2586ee2ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "458/458 [==============================] - ETA: 0s - loss: 0.4544 - custom_sparse_categorical_accuracy: 0.8182\n",
            "Epoch 1: saving model to ./trained_model/weights.ckpt\n",
            "458/458 [==============================] - 16s 8ms/step - loss: 0.4544 - custom_sparse_categorical_accuracy: 0.8182\n",
            "Epoch 2/20\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.2637 - custom_sparse_categorical_accuracy: 0.8989\n",
            "Epoch 2: saving model to ./trained_model/weights.ckpt\n",
            "458/458 [==============================] - 4s 8ms/step - loss: 0.2627 - custom_sparse_categorical_accuracy: 0.8990\n",
            "Epoch 3/20\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.2225 - custom_sparse_categorical_accuracy: 0.9154\n",
            "Epoch 3: saving model to ./trained_model/weights.ckpt\n",
            "458/458 [==============================] - 4s 8ms/step - loss: 0.2221 - custom_sparse_categorical_accuracy: 0.9156\n",
            "Epoch 4/20\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.1886 - custom_sparse_categorical_accuracy: 0.9345\n",
            "Epoch 4: saving model to ./trained_model/weights.ckpt\n",
            "458/458 [==============================] - 4s 8ms/step - loss: 0.1878 - custom_sparse_categorical_accuracy: 0.9349\n",
            "Epoch 5/20\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.1362 - custom_sparse_categorical_accuracy: 0.9521\n",
            "Epoch 5: saving model to ./trained_model/weights.ckpt\n",
            "458/458 [==============================] - 4s 8ms/step - loss: 0.1362 - custom_sparse_categorical_accuracy: 0.9519\n",
            "Epoch 6/20\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.1518 - custom_sparse_categorical_accuracy: 0.9503\n",
            "Epoch 6: saving model to ./trained_model/weights.ckpt\n",
            "458/458 [==============================] - 4s 8ms/step - loss: 0.1519 - custom_sparse_categorical_accuracy: 0.9502\n",
            "Epoch 7/20\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.1249 - custom_sparse_categorical_accuracy: 0.9626\n",
            "Epoch 7: saving model to ./trained_model/weights.ckpt\n",
            "458/458 [==============================] - 4s 8ms/step - loss: 0.1244 - custom_sparse_categorical_accuracy: 0.9628\n",
            "Epoch 8/20\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0824 - custom_sparse_categorical_accuracy: 0.9741\n",
            "Epoch 8: saving model to ./trained_model/weights.ckpt\n",
            "458/458 [==============================] - 4s 8ms/step - loss: 0.0820 - custom_sparse_categorical_accuracy: 0.9742\n",
            "Epoch 9/20\n",
            "453/458 [============================>.] - ETA: 0s - loss: 0.0660 - custom_sparse_categorical_accuracy: 0.9757\n",
            "Epoch 9: saving model to ./trained_model/weights.ckpt\n",
            "458/458 [==============================] - 4s 8ms/step - loss: 0.0654 - custom_sparse_categorical_accuracy: 0.9760\n",
            "Epoch 10/20\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.1028 - custom_sparse_categorical_accuracy: 0.9701\n",
            "Epoch 10: saving model to ./trained_model/weights.ckpt\n",
            "458/458 [==============================] - 4s 8ms/step - loss: 0.1024 - custom_sparse_categorical_accuracy: 0.9703\n",
            "Epoch 11/20\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0459 - custom_sparse_categorical_accuracy: 0.9838\n",
            "Epoch 11: saving model to ./trained_model/weights.ckpt\n",
            "458/458 [==============================] - 4s 8ms/step - loss: 0.0458 - custom_sparse_categorical_accuracy: 0.9838\n",
            "Epoch 12/20\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0336 - custom_sparse_categorical_accuracy: 0.9895\n",
            "Epoch 12: saving model to ./trained_model/weights.ckpt\n",
            "458/458 [==============================] - 4s 8ms/step - loss: 0.0338 - custom_sparse_categorical_accuracy: 0.9891\n",
            "Epoch 13/20\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0682 - custom_sparse_categorical_accuracy: 0.9798\n",
            "Epoch 13: saving model to ./trained_model/weights.ckpt\n",
            "458/458 [==============================] - 4s 8ms/step - loss: 0.0681 - custom_sparse_categorical_accuracy: 0.9799\n",
            "Epoch 14/20\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.1267 - custom_sparse_categorical_accuracy: 0.9741\n",
            "Epoch 14: saving model to ./trained_model/weights.ckpt\n",
            "458/458 [==============================] - 4s 8ms/step - loss: 0.1263 - custom_sparse_categorical_accuracy: 0.9742\n",
            "Epoch 15/20\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0685 - custom_sparse_categorical_accuracy: 0.9768\n",
            "Epoch 15: saving model to ./trained_model/weights.ckpt\n",
            "458/458 [==============================] - 4s 8ms/step - loss: 0.0683 - custom_sparse_categorical_accuracy: 0.9768\n",
            "Epoch 16/20\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0283 - custom_sparse_categorical_accuracy: 0.9917\n",
            "Epoch 16: saving model to ./trained_model/weights.ckpt\n",
            "458/458 [==============================] - 4s 8ms/step - loss: 0.0282 - custom_sparse_categorical_accuracy: 0.9917\n",
            "Epoch 17/20\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0333 - custom_sparse_categorical_accuracy: 0.9868\n",
            "Epoch 17: saving model to ./trained_model/weights.ckpt\n",
            "458/458 [==============================] - 4s 8ms/step - loss: 0.0332 - custom_sparse_categorical_accuracy: 0.9869\n",
            "Epoch 18/20\n",
            "455/458 [============================>.] - ETA: 0s - loss: 0.0463 - custom_sparse_categorical_accuracy: 0.9859\n",
            "Epoch 18: saving model to ./trained_model/weights.ckpt\n",
            "458/458 [==============================] - 4s 8ms/step - loss: 0.0461 - custom_sparse_categorical_accuracy: 0.9860\n",
            "Epoch 19/20\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0241 - custom_sparse_categorical_accuracy: 0.9908\n",
            "Epoch 19: saving model to ./trained_model/weights.ckpt\n",
            "458/458 [==============================] - 4s 8ms/step - loss: 0.0240 - custom_sparse_categorical_accuracy: 0.9908\n",
            "Epoch 20/20\n",
            "456/458 [============================>.] - ETA: 0s - loss: 0.0165 - custom_sparse_categorical_accuracy: 0.9969\n",
            "Epoch 20: saving model to ./trained_model/weights.ckpt\n",
            "458/458 [==============================] - 4s 8ms/step - loss: 0.0164 - custom_sparse_categorical_accuracy: 0.9969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.plot(range(epochs), accuracy, 'r--', label='train')\n",
        "plt.title('Training accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "y202ek6aWQ--",
        "outputId": "53811a25-6de9-4cb8-ebea-6b3a62efd8dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregando um model salvo"
      ],
      "metadata": {
        "id": "TkvCdPezJl5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd  '/content/drive/MyDrive/curso_analise_imagens_satelite/minas'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-zEr2yh_zRF",
        "outputId": "91fcc0b3-5009-465a-f6f3-7af720860aaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/curso_analise_imagens_satelite/minas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists('./trained_model/checkpoint'): \n",
        "  metadata = np.load('./modelMetaData.npz',allow_pickle=True)\n",
        "  image_width,image_height,Nbands,n_classes,char_labels = metadata['metadata'] \n",
        "\n",
        "  model = define_FCN_discovery(image_width,image_height,Nbands)\n",
        "  \n",
        "  model.load_weights('./trained_model/weights.ckpt')\n",
        "    \n",
        "  npzfile = np.load('./trainingDataAvgStd.npz',)\n",
        "  nmean = npzfile['median']\n",
        "  nmax = npzfile['std']"
      ],
      "metadata": {
        "id": "aCyIdWwrJo-U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c0216b-45d9-46a3-bbb3-8892f3e7a401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None, None, 7)]   0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, None, None, 32)    2048      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, None, None, 32)   0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, None, None, 64)    18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, None, None, 64)   0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, None, None, 64)    36928     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, None, None, 64)   0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, None, None, 64)    200768    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, None, 64)    0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, None, None, 2)     130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 258,370\n",
            "Trainable params: 258,370\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testando o modelo "
      ],
      "metadata": {
        "id": "fNMqMTAgJ9pK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = model.predict(train_images)\n",
        "\n"
      ],
      "metadata": {
        "id": "Dfv0HjxLZ1Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "predics = np.zeros(len(prediction))\n",
        "\n",
        "predics = prediction[:,0,0,:].argmax(axis=1) \n",
        "\n",
        "print(confusion_matrix(train_labels, predics))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcKfd_dLbYsZ",
        "outputId": "19861b3b-8e3b-4fd0-f7c4-ba49a3772cdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1114    1]\n",
            " [   1 1172]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "print(\"Testing:\")\n",
        "prediction = model.predict(test_images)\n",
        "\n",
        "predics = np.zeros(len(prediction))\n",
        "\n",
        "predics = prediction[:,0,0,:].argmax(axis=1) \n",
        "\n",
        "print(confusion_matrix(test_labels, predics))\n",
        "\n",
        "y_true = test_labels\n",
        "y_pred = predics\n",
        "\n",
        "if n_classes==2:\n",
        "  print(\"Precision: \", metrics.precision_score(y_true, y_pred))\n",
        "  print(\"Recall: \", metrics.recall_score(y_true, y_pred))\n",
        "  print(\"F1 score: \", metrics.f1_score(y_true, y_pred))\n",
        "  print(\"Mean accuracy: \", metrics.accuracy_score(y_true, y_pred))\n",
        "  print(\"AUC: \", metrics.roc_auc_score(y_true, y_pred))\n",
        "  print(\"mcc: \", metrics.matthews_corrcoef(y_true, y_pred))\n",
        "  print(\"kappa: \", metrics.cohen_kappa_score(y_true, y_pred))\n"
      ],
      "metadata": {
        "id": "pYymY3CpJ7mt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eb64a78-b175-41da-e8b7-65badbd9723a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing:\n",
            "[[274   8]\n",
            " [  6 284]]\n",
            "Precision:  0.9726027397260274\n",
            "Recall:  0.9793103448275862\n",
            "F1 score:  0.9759450171821304\n",
            "Mean accuracy:  0.9755244755244755\n",
            "AUC:  0.9754707752506725\n",
            "mcc:  0.9510578522874463\n",
            "kappa:  0.9510345839651715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgXd8fGorPxz"
      },
      "source": [
        "# Aplicando o modelo em grandes imagens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(modelo,labels,experiment_name,image_width,\n",
        "            image_height,nmean,nmax,Nbands,\n",
        "            path_to_predict_images,path_to_results,\n",
        "            slicing = 1):\n",
        "\n",
        "  \"\"\"\n",
        "    path_to_predict_images: onde ficam os rasters das imagens\n",
        "    slicing: parâmetro que diz em quantos pedaços quebrar\n",
        "      1, pega imagem inteira\n",
        "      2: divide em 4 blocos\n",
        "  \"\"\"\n",
        "  countImages = 0\n",
        "  \n",
        "  print(\"Escolha a classe alvo: \")\n",
        "  for i in range(0, len(labels)):\n",
        "    print(i,\":\",labels[i])\n",
        "  try:\n",
        "    save=int(input('Input:'))\n",
        "  except ValueError:\n",
        "    print(\"Not a number\")\n",
        "\n",
        "  kmlFull=simplekml.Kml()\n",
        "  resultsFull = [] \n",
        "\n",
        "  # processa cada imagem no diretório de predição\n",
        "  for file in os.listdir(path_to_predict_images):\n",
        "    print(path_to_predict_images + file)\n",
        "\n",
        "    starthere = len(resultsFull)\n",
        "    \n",
        "    # gdal lê o tiff\n",
        "    testimage = gdal.Translate(NamedTemporaryFile(delete=False).name,\n",
        "                                  gdal.Open(path_to_predict_images + file, gdal.GA_ReadOnly),\n",
        "                                  options = '-b 2 -b 3 -b 4 -b 5 -b 7 -b 8 -b 11')           # use as mesmas bandas\n",
        "\n",
        "    print(\"imagem carregada\")\n",
        "    testimage = np.transpose(np.array(testimage.ReadAsArray(),dtype = 'float32'), axes=(1, 2, 0))\n",
        "    testimage = testimage[:,:,0:Nbands]\n",
        "    for i in range(Nbands):\n",
        "      testimage[:,:,i]=testimage[:,:,i]-nmean[i]\n",
        "      testimage[:,:,i]=testimage[:,:,i]/nmax[i]\n",
        "\n",
        "    # corta a imagem para previsão se for muito grande para o colab. \n",
        "    # divide a imagem em patches regulares 2D \n",
        "    # Adapte o \"slicing\" ao seu tamanho de imagem e disponibilidade de memória no colab  \n",
        "    \n",
        "    sh = np.array(testimage.shape)\n",
        "    sh[0] = sh[0]//slicing\n",
        "    sh[1] = sh[1]//slicing\n",
        "    prediction = None\n",
        "\n",
        "    print(\"imagem processada\")\n",
        "\n",
        "    for i in range(0,slicing):  \n",
        "      for j in range(0,slicing):\n",
        "        patch = np.expand_dims(testimage[sh[0]*i:sh[0]*(i+1),sh[1]*j:sh[1]*(j+1),:],axis=0) \n",
        "        # compute prediction of the FCN\n",
        "        if prediction is None:\n",
        "          prediction = model.predict(patch)\n",
        "        else:\n",
        "          prediction = np.append(prediction,model.predict(patch), axis=0)\n",
        "      \n",
        "    # locate the predictions on the patches\n",
        "\n",
        "    shp = prediction.shape\n",
        "    results = []\n",
        "    lpatches = []\n",
        "    kml=simplekml.Kml()\n",
        "    count_positives = 0\n",
        "\n",
        "    with rasterio.open(path_to_predict_images+file) as map_layer:\n",
        "      for i in range(0,slicing):  \n",
        "        for j in range(0,slicing): \n",
        " \n",
        "          print('slice:',i,j)\n",
        "          pos = i*slicing+j\n",
        "          \n",
        "          pred_classes = tf.greater(prediction[pos,:,:,save],0.6) # considerando um limite de probabilidade de 60% para aceitar um resultado\n",
        "          shpc = tf.shape(pred_classes)\n",
        "\n",
        "          inds = tf.where(pred_classes==True)\n",
        "\n",
        "          # recalculando a posição para a dimensão original da imagem\n",
        "          # gera mapas menores\n",
        "          xloc = i*sh[0]+np.array(tf.gather(np.linspace(image_width/2.0,sh[0]-image_width/2.0,num=shpc[0]),inds[:,0]))\n",
        "          yloc = j*sh[1]+np.array(tf.gather(np.linspace(image_height/2.0,sh[1]-image_height/2.0,num=shpc[1]),inds[:,1]))\n",
        "          predloc = prediction[pos,inds[:,0],inds[:,1],save]\n",
        "\n",
        "          for k in range(0,len(xloc)):\n",
        "            pixels2coords = map_layer.xy(xloc[k] , yloc[k]) \n",
        "\n",
        "            r = np.ones(4)*-1      \n",
        "            \n",
        "            r[0] = pixels2coords[0]\n",
        "            r[1] = pixels2coords[1]\n",
        "            r[2] = predloc[k]\n",
        "\n",
        "            tag = labels[save]\n",
        "              \n",
        "            kml.newpoint(name=tag, coords=[(pixels2coords[0],pixels2coords[1])])\n",
        "            kmlFull.newpoint(name=tag, coords=[(pixels2coords[0],pixels2coords[1])])\n",
        "\n",
        "            results.append(r)\n",
        "            resultsFull.append(r)\n",
        "\n",
        "    # save prediction files to the results folder\n",
        "    np.savetxt(path_to_results + 'prediction_{}.csv'.format(file), results, delimiter=\",\")\n",
        "    kml.save(path_to_results + 'predictions_{}.kml'.format(file))  \n",
        "\n",
        "    countImages+=1\n",
        "\n",
        "    del testimage\n",
        "    del patch\n",
        "    del prediction\n",
        "    del results\n",
        "    del kml\n",
        "    gc.collect()\n",
        "\n",
        "  np.savetxt(path_to_results + 'prediction_{}.csv'.format(experiment_name), np.array(resultsFull), delimiter=\",\")\n",
        "  kmlFull.save(path_to_results + 'predictions_{}.kml'.format(experiment_name))  \n"
      ],
      "metadata": {
        "id": "T4BrW-WRbQ6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ajuste os diretórios"
      ],
      "metadata": {
        "id": "hinQYPLbAPYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_results =   '/content/drive/MyDrive/curso_analise_imagens_satelite/minas/'\n",
        "path_to_predict_images = '/content/drive/MyDrive/DataForTailingDamDetection/Data/SentinelMinesDams/predict/'\n",
        "\n",
        "predict(model,char_labels,'minas',image_width,image_height,nmean,nmax,Nbands,path_to_predict_images,path_to_results,slicing=10)"
      ],
      "metadata": {
        "id": "qla53ZZg_EM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aa1615b-638b-4952-ae99-2dcd11739ce6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Escolha a classe alvo: \n",
            "0 : mine\n",
            "1 : notmine\n",
            "Input:0\n",
            "/content/drive/MyDrive/DataForTailingDamDetection/Data/SentinelMinesDams/predict/retanguloMGSAR-0000000000-0000018432.tif\n",
            "imagem carregada\n",
            "imagem processada\n",
            "slice: 0 0\n",
            "slice: 0 1\n",
            "slice: 0 2\n",
            "slice: 0 3\n",
            "slice: 0 4\n",
            "slice: 0 5\n",
            "slice: 0 6\n",
            "slice: 0 7\n",
            "slice: 0 8\n",
            "slice: 0 9\n",
            "slice: 1 0\n",
            "slice: 1 1\n",
            "slice: 1 2\n",
            "slice: 1 3\n",
            "slice: 1 4\n",
            "slice: 1 5\n",
            "slice: 1 6\n",
            "slice: 1 7\n",
            "slice: 1 8\n",
            "slice: 1 9\n",
            "slice: 2 0\n",
            "slice: 2 1\n",
            "slice: 2 2\n",
            "slice: 2 3\n",
            "slice: 2 4\n",
            "slice: 2 5\n",
            "slice: 2 6\n",
            "slice: 2 7\n",
            "slice: 2 8\n",
            "slice: 2 9\n",
            "slice: 3 0\n",
            "slice: 3 1\n",
            "slice: 3 2\n",
            "slice: 3 3\n",
            "slice: 3 4\n",
            "slice: 3 5\n",
            "slice: 3 6\n",
            "slice: 3 7\n",
            "slice: 3 8\n",
            "slice: 3 9\n",
            "slice: 4 0\n",
            "slice: 4 1\n",
            "slice: 4 2\n",
            "slice: 4 3\n",
            "slice: 4 4\n",
            "slice: 4 5\n",
            "slice: 4 6\n",
            "slice: 4 7\n",
            "slice: 4 8\n",
            "slice: 4 9\n",
            "slice: 5 0\n",
            "slice: 5 1\n",
            "slice: 5 2\n",
            "slice: 5 3\n",
            "slice: 5 4\n",
            "slice: 5 5\n",
            "slice: 5 6\n",
            "slice: 5 7\n",
            "slice: 5 8\n",
            "slice: 5 9\n",
            "slice: 6 0\n",
            "slice: 6 1\n",
            "slice: 6 2\n",
            "slice: 6 3\n",
            "slice: 6 4\n",
            "slice: 6 5\n",
            "slice: 6 6\n",
            "slice: 6 7\n",
            "slice: 6 8\n",
            "slice: 6 9\n",
            "slice: 7 0\n",
            "slice: 7 1\n",
            "slice: 7 2\n",
            "slice: 7 3\n",
            "slice: 7 4\n",
            "slice: 7 5\n",
            "slice: 7 6\n",
            "slice: 7 7\n",
            "slice: 7 8\n",
            "slice: 7 9\n",
            "slice: 8 0\n",
            "slice: 8 1\n",
            "slice: 8 2\n",
            "slice: 8 3\n",
            "slice: 8 4\n",
            "slice: 8 5\n",
            "slice: 8 6\n",
            "slice: 8 7\n",
            "slice: 8 8\n",
            "slice: 8 9\n",
            "slice: 9 0\n",
            "slice: 9 1\n",
            "slice: 9 2\n",
            "slice: 9 3\n",
            "slice: 9 4\n",
            "slice: 9 5\n",
            "slice: 9 6\n",
            "slice: 9 7\n",
            "slice: 9 8\n",
            "slice: 9 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROODv-k5gnr8"
      },
      "source": [
        "## A previsão gera arquivos csv e kml com coordenadas dos pontos encontrados"
      ]
    }
  ]
}