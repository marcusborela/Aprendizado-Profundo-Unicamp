{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deepLearningSentinel2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcusborela/Aprendizado-Profundo-Unicamp/blob/main/deepLearningSentinel2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE8_YNK7eSdi"
      },
      "source": [
        "# Mines and tailings dams dicovery and classification\n",
        "\n",
        "This script is part of a research project published on the paper \"Mining and Tailings Dam Detection In Satellite Imagery Using Deep Learning\" by Remis Balaniuk, Olga Isupova and Steven Reece. This project was developed at the University of Oxford from September 2019 to February 2020.\n",
        "It was prepared to be used on the Google Colaboratory platform (see https://colab.research.google.com/notebooks/welcome.ipynb ).  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZN7-cp2euZk"
      },
      "source": [
        "Use GPU acceleration (Runtime >> Change runtime type) !!\n",
        "\n",
        "Initial settings: import libraries and mount Google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqo4fDgNeEpX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c56cad3a-db2f-4e89-9ff2-52efc171282b"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "!pip install rasterio\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "!pip install simplekml\n",
        "import simplekml\n",
        "import psutil\n",
        "import humanize\n",
        "import GPUtil as GPU\n",
        "import rasterio\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import gc\n",
        "import glob\n",
        "import pickle as pi\n",
        "import re\n",
        "import math\n",
        "from random import shuffle\n",
        "import gdal\n",
        "import os\n",
        "from tempfile import NamedTemporaryFile\n",
        "import h5py\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,f1_score\n",
        "from google.colab import drive\n",
        "from IPython.display import IFrame\n",
        "\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "rseed = 100\n",
        "np.random.seed(rseed)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rasterio\n",
            "  Downloading rasterio-1.2.10-cp37-cp37m-manylinux1_x86_64.whl (19.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.3 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from rasterio) (21.4.0)\n",
            "Collecting snuggs>=1.4.1\n",
            "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
            "Collecting cligj>=0.5\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Collecting click-plugins\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rasterio) (57.4.0)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.7/dist-packages (from rasterio) (7.1.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from rasterio) (2022.5.18.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rasterio) (1.21.6)\n",
            "Collecting affine\n",
            "  Downloading affine-2.3.1-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.7/dist-packages (from snuggs>=1.4.1->rasterio) (3.0.9)\n",
            "Installing collected packages: snuggs, cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.3.1 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.2.10 snuggs-1.4.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gputil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=ba6fab6eb83530833adca6642e0d179c0168e76ea5823d03d030e3b2a168d81a\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting simplekml\n",
            "  Downloading simplekml-1.3.6.tar.gz (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: simplekml\n",
            "  Building wheel for simplekml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for simplekml: filename=simplekml-1.3.6-py3-none-any.whl size=65876 sha256=6c82f71c18051f90dc783d08e7c307ecfc97bcfcd7fa4894a4b9c4ac016cd9df\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/ec/e6/10af1a1fb29ffca95151d4c886d6e06fc309c68f46519892de\n",
            "Successfully built simplekml\n",
            "Installing collected packages: simplekml\n",
            "Successfully installed simplekml-1.3.6\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b3jhzTqe8Z4"
      },
      "source": [
        "## Define environment for I/O:\n",
        "\n",
        "This script assumes that the user created on his google drive (mounted as /content/drive/My Drive/) a folder named \"DataForTailingDamDetection/\".\n",
        "\n",
        "Inside this folder three subfoders willl be used: Data/, Experiments/ and Results/. The first one should be used to store images for training, testing and predicting, uploaded by the user. The second will be used to store the trained models and the last one will keep predicting results. \n",
        "\n",
        "The analytical approach is organized in \"experiments\". Each experiment can have its own dataset, model and results. An experiment will have its own subfoder on each of the three main folders: Data/, Experiments/ and Results/. \n",
        "\n",
        "For each new experiment the user must create a new subfolder under the folder  Data/ named after his experiment. Inside this Data/\\<experiment name\\>/ folder the user will create two subfolders: train/ and predict/. The train/ folder must be used to put all training and testing images. The images must be organized in subfolders, with one subfolder for each class. On Data/\\<experiment name\\>/predict/ the user will store the large satellite images for which predicting will be done.\n",
        "\n",
        "The remaining folders will be created automatically by the script.\n",
        "\n",
        "Two experiments were described on the paper  \"Mining and Tailings Dam Detection In Satellite Imagery Using Deep Learning\". The first implements mines and tailings dams discovery and the second classifies the ore type of discovery mines and tailings dams. This is the folder's tree for the project experiments:\n",
        "\n",
        "![](https://drive.google.com/file/d/1GdqNvfEzfb5JCG7YYirThg9aUHl8TM26/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i01G_2kle5Vd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dab57a9c-048b-4e0c-c18d-45a68446a573"
      },
      "source": [
        " %cd /content/drive/My Drive/\n",
        "\n",
        "# root folder for the project data\n",
        "path_ref = './curso_analise_imagens_satelite/FinalLixoes/'\n",
        "sys.path.extend([path_ref])\n",
        "\n",
        "data_path = 'Data/'\n",
        "exp_names = []\n",
        "\n",
        "#-----------------------------------------------\n",
        "# user definitions\n",
        "MINE_IMAGE_WIDTH = 40 # 400 x 400\n",
        "MINE_IMAGE_HEIGHT = 40\n",
        "#ORE_IMAGE_WIDTH = 21\n",
        "#ORE_IMAGE_HEIGHT = 21\n",
        "N_BANDS = 7\n",
        "# names of the image folders\n",
        "experiment_name = ''  \n",
        "# oretypeclassifier = 'SentinelOreType' # set to None if ore classifier is not required\n",
        "#-----------------------------------------------\n",
        "\n",
        "experiments_path = 'Experiments/'\n",
        "result_path = 'Results/'\n",
        "if not os.path.exists(path_ref + experiments_path):\n",
        "    os.makedirs(path_ref + experiments_path)\n",
        "if not os.path.exists(path_ref + result_path):\n",
        "    os.makedirs(path_ref + result_path)\n",
        "\n",
        "train_data = 'train/'   \n",
        "predict_data =  'predict/' \n",
        "\n",
        "# user must create the train_data folder and put the training images on subfolders (one per class)\n",
        "path_to_images = path_ref + data_path + experiment_name + '/' + train_data \n",
        "#path_to_oretype_samples = path_ref + data_path + oretypeclassifier + '/' + train_data \n",
        "\n",
        "# if user wants to use model to predict on large images he must create the predict folder and put the images \n",
        "path_to_predict_images = path_ref + data_path + experiment_name + '/' + predict_data\n",
        "\n",
        "# will save training results here\n",
        "path_to_output = path_ref + experiments_path + experiment_name + '/'\n",
        "# path_to_oretype_output = path_ref + experiments_path + oretypeclassifier + '/'\n",
        "\n",
        "#will save prediction results here\n",
        "path_to_results = path_ref + result_path + experiment_name + '/'\n",
        "if not os.path.exists(path_to_results):\n",
        "    os.makedirs(path_to_results)\n",
        "if not os.path.exists(path_to_output):\n",
        "    os.makedirs(path_to_output)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMlwsw_Yhwrb"
      },
      "source": [
        "Before following this script make sure that the images for training were uploaded to the \"path_to_images\" folder. For each class define a subfoder to store its images. To obtain the images on Google Earth Engine use the loadminetailingsdam.ipynb notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"content/drive\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tr_FzKYgy7Ti",
        "outputId": "90ab0f4d-5a3e-4c58-e03f-7a22dc4b8f46"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'content/drive'\n",
            "/content/drive/My Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DalJhLHjy_8w",
        "outputId": "fa2debf3-dcc5-479c-d334-65f7543fd8e4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1_4911225524505804922.epub\n",
            " \u001b[0m\u001b[01;34mA_converter\u001b[0m/\n",
            "\u001b[01;34m'Auto Call Recorder'\u001b[0m/\n",
            "'Cartão de embarque | LATAM Airlines no Brasil.pdf'\n",
            " \u001b[01;34mClassroom\u001b[0m/\n",
            "\u001b[01;34m'Colab Notebooks'\u001b[0m/\n",
            " coordenadas_lixoeslixao100.tif\n",
            " coordenadas_lixoeslixao101.tif\n",
            " coordenadas_lixoeslixao102.tif\n",
            " coordenadas_lixoeslixao103.tif\n",
            " coordenadas_lixoeslixao104.tif\n",
            " coordenadas_lixoeslixao105.tif\n",
            " coordenadas_lixoeslixao106.tif\n",
            " coordenadas_lixoeslixao107.tif\n",
            " coordenadas_lixoeslixao108.tif\n",
            " coordenadas_lixoeslixao109.tif\n",
            " coordenadas_lixoeslixao110.tif\n",
            " coordenadas_lixoeslixao111.tif\n",
            " coordenadas_lixoeslixao112.tif\n",
            " coordenadas_lixoeslixao113.tif\n",
            " coordenadas_lixoeslixao114.tif\n",
            " coordenadas_lixoeslixao115.tif\n",
            " coordenadas_lixoeslixao116.tif\n",
            " coordenadas_lixoeslixao117.tif\n",
            " coordenadas_lixoeslixao118.tif\n",
            " coordenadas_lixoeslixao119.tif\n",
            " coordenadas_lixoeslixao120.tif\n",
            " coordenadas_lixoeslixao121.tif\n",
            " coordenadas_lixoeslixao122.tif\n",
            " coordenadas_lixoeslixao123.tif\n",
            " coordenadas_lixoeslixao124.tif\n",
            " coordenadas_lixoeslixao125.tif\n",
            " coordenadas_lixoeslixao126.tif\n",
            " coordenadas_lixoeslixao127.tif\n",
            " coordenadas_lixoeslixao128.tif\n",
            " coordenadas_lixoeslixao129.tif\n",
            " coordenadas_lixoeslixao130.tif\n",
            " coordenadas_lixoeslixao131.tif\n",
            " coordenadas_lixoeslixao132.tif\n",
            " coordenadas_lixoeslixao133.tif\n",
            " coordenadas_lixoeslixao134.tif\n",
            " coordenadas_lixoeslixao135.tif\n",
            " coordenadas_lixoeslixao136.tif\n",
            " coordenadas_lixoeslixao137.tif\n",
            " coordenadas_lixoeslixao138.tif\n",
            " coordenadas_lixoeslixao139.tif\n",
            " coordenadas_lixoeslixao140.tif\n",
            " coordenadas_lixoeslixao141.tif\n",
            " coordenadas_lixoeslixao142.tif\n",
            " coordenadas_lixoeslixao143.tif\n",
            " coordenadas_lixoeslixao144.tif\n",
            " coordenadas_lixoeslixao145.tif\n",
            " coordenadas_lixoeslixao146.tif\n",
            " coordenadas_lixoeslixao147.tif\n",
            " coordenadas_lixoeslixao148.tif\n",
            " coordenadas_lixoeslixao149.tif\n",
            " coordenadas_lixoeslixao150.tif\n",
            " coordenadas_lixoeslixao151.tif\n",
            " coordenadas_lixoeslixao152.tif\n",
            " coordenadas_lixoeslixao153.tif\n",
            " coordenadas_lixoeslixao154.tif\n",
            " coordenadas_lixoeslixao155.tif\n",
            " coordenadas_lixoeslixao156.tif\n",
            " coordenadas_lixoeslixao157.tif\n",
            " coordenadas_lixoeslixao158.tif\n",
            " coordenadas_lixoeslixao159.tif\n",
            " coordenadas_lixoeslixao160.tif\n",
            " coordenadas_lixoeslixao161.tif\n",
            " coordenadas_lixoeslixao162.tif\n",
            " coordenadas_lixoeslixao163.tif\n",
            " coordenadas_lixoeslixao164.tif\n",
            " coordenadas_lixoeslixao165.tif\n",
            " coordenadas_lixoeslixao166.tif\n",
            " coordenadas_lixoeslixao167.tif\n",
            " coordenadas_lixoeslixao168.tif\n",
            " coordenadas_lixoeslixao169.tif\n",
            " coordenadas_lixoeslixao170.tif\n",
            " coordenadas_lixoeslixao171.tif\n",
            " coordenadas_lixoeslixao172.tif\n",
            " coordenadas_lixoeslixao173.tif\n",
            " coordenadas_lixoeslixao174.tif\n",
            " coordenadas_lixoeslixao175.tif\n",
            " coordenadas_lixoeslixao176.tif\n",
            " coordenadas_lixoeslixao177.tif\n",
            " coordenadas_lixoeslixao178.tif\n",
            " coordenadas_lixoeslixao179.tif\n",
            " coordenadas_lixoeslixao180.tif\n",
            " coordenadas_lixoeslixao181.tif\n",
            " coordenadas_lixoeslixao182.tif\n",
            " coordenadas_lixoeslixao183.tif\n",
            " coordenadas_lixoeslixao184.tif\n",
            " coordenadas_lixoeslixao185.tif\n",
            " coordenadas_lixoeslixao186.tif\n",
            " coordenadas_lixoeslixao187.tif\n",
            " coordenadas_lixoeslixao188.tif\n",
            " coordenadas_lixoeslixao189.tif\n",
            " coordenadas_lixoeslixao190.tif\n",
            " coordenadas_lixoeslixao191.tif\n",
            " coordenadas_lixoeslixao192.tif\n",
            " coordenadas_lixoeslixao193.tif\n",
            " coordenadas_lixoeslixao194.tif\n",
            " coordenadas_lixoeslixao195.tif\n",
            " coordenadas_lixoeslixao196.tif\n",
            " coordenadas_lixoeslixao197.tif\n",
            " coordenadas_lixoeslixao198.tif\n",
            " coordenadas_lixoeslixao199.tif\n",
            " coordenadas_lixoeslixao200.tif\n",
            " coordenadas_lixoeslixao201.tif\n",
            " coordenadas_lixoeslixao202.tif\n",
            " coordenadas_lixoeslixao203.tif\n",
            " coordenadas_lixoeslixao204.tif\n",
            " coordenadas_lixoeslixao205.tif\n",
            " coordenadas_lixoeslixao206.tif\n",
            " coordenadas_lixoeslixao207.tif\n",
            " coordenadas_lixoeslixao208.tif\n",
            " coordenadas_lixoeslixao209.tif\n",
            " coordenadas_lixoeslixao210.tif\n",
            " coordenadas_lixoeslixao211.tif\n",
            " coordenadas_lixoeslixao212.tif\n",
            " coordenadas_lixoeslixao213.tif\n",
            " coordenadas_lixoeslixao214.tif\n",
            " coordenadas_lixoeslixao215.tif\n",
            " coordenadas_lixoeslixao216.tif\n",
            " coordenadas_lixoeslixao217.tif\n",
            " coordenadas_lixoeslixao218.tif\n",
            " coordenadas_lixoeslixao219.tif\n",
            " coordenadas_lixoeslixao220.tif\n",
            " coordenadas_lixoeslixao221.tif\n",
            " coordenadas_lixoeslixao222.tif\n",
            " coordenadas_lixoeslixao223.tif\n",
            " coordenadas_lixoeslixao224.tif\n",
            " coordenadas_lixoeslixao225.tif\n",
            " coordenadas_lixoeslixao226.tif\n",
            " coordenadas_lixoeslixao227.tif\n",
            " coordenadas_lixoeslixao228.tif\n",
            " coordenadas_lixoeslixao229.tif\n",
            " coordenadas_lixoeslixao230.tif\n",
            " coordenadas_lixoeslixao231.tif\n",
            " coordenadas_lixoeslixao232.tif\n",
            " coordenadas_lixoeslixao233.tif\n",
            " coordenadas_lixoeslixao234.tif\n",
            " coordenadas_lixoeslixao235.tif\n",
            " coordenadas_lixoeslixao236.tif\n",
            " coordenadas_lixoeslixao237.tif\n",
            " coordenadas_lixoeslixao238.tif\n",
            " coordenadas_lixoeslixao239.tif\n",
            " coordenadas_lixoeslixao240.tif\n",
            " coordenadas_lixoeslixao241.tif\n",
            " coordenadas_lixoeslixao242.tif\n",
            " coordenadas_lixoeslixao243.tif\n",
            " coordenadas_lixoeslixao244.tif\n",
            " coordenadas_lixoeslixao245.tif\n",
            " coordenadas_lixoeslixao246.tif\n",
            " coordenadas_lixoeslixao247.tif\n",
            " coordenadas_lixoeslixao248.tif\n",
            " coordenadas_lixoeslixao249.tif\n",
            " coordenadas_lixoeslixao250.tif\n",
            " coordenadas_lixoeslixao251.tif\n",
            " coordenadas_lixoeslixao252.tif\n",
            " coordenadas_lixoeslixao253.tif\n",
            " coordenadas_lixoeslixao254.tif\n",
            " coordenadas_lixoeslixao255.tif\n",
            " coordenadas_lixoeslixao256.tif\n",
            " coordenadas_lixoeslixao257.tif\n",
            " coordenadas_lixoeslixao258.tif\n",
            " coordenadas_lixoeslixao259.tif\n",
            " coordenadas_lixoeslixao260.tif\n",
            " coordenadas_lixoeslixao261.tif\n",
            " coordenadas_lixoeslixao262.tif\n",
            " coordenadas_lixoeslixao263.tif\n",
            " coordenadas_lixoeslixao264.tif\n",
            " coordenadas_lixoeslixao265.tif\n",
            " coordenadas_lixoeslixao266.tif\n",
            " coordenadas_lixoeslixao267.tif\n",
            " coordenadas_lixoeslixao268.tif\n",
            " coordenadas_lixoeslixao269.tif\n",
            " coordenadas_lixoeslixao91.tif\n",
            " coordenadas_lixoeslixao92.tif\n",
            " coordenadas_lixoeslixao93.tif\n",
            " coordenadas_lixoeslixao94.tif\n",
            " coordenadas_lixoeslixao95.tif\n",
            " coordenadas_lixoeslixao96.tif\n",
            " coordenadas_lixoeslixao97.tif\n",
            " coordenadas_lixoeslixao98.tif\n",
            " coordenadas_lixoeslixao99.tif\n",
            " \u001b[01;34mcurso_analise_imagens_satelite\u001b[0m/\n",
            " dam0.tif\n",
            " dam1.tif\n",
            " dam2.tif\n",
            " dam3.tif\n",
            " dam4.tif\n",
            " dam5.tif\n",
            " dam6.tif\n",
            " dam7.tif\n",
            " dam8.tif\n",
            " \u001b[01;36mDataForTailingDamDetection\u001b[0m@\n",
            " \u001b[01;36mdogs-and-cats\u001b[0m@\n",
            "'Exercícios - 20210718 - Marcus Vinícius Borela de Castro'\n",
            " foo.txt\n",
            " \u001b[01;34mia025\u001b[0m/\n",
            " \u001b[01;34mIgreja\u001b[0m/\n",
            " \u001b[01;34mmarcus_pc_bkup\u001b[0m/\n",
            " Quorum_reuniões_anteriores.xlsx\n",
            " \u001b[01;34mrastersTreinamento\u001b[0m/\n",
            " Relatório_Final_Robustez_Consulta.gdoc\n",
            " Relatório_Final_Robustez_Query.gdoc\n",
            "'Sample file.txt'\n",
            " \u001b[01;34mSEINT\u001b[0m/\n",
            " \u001b[01;34mTCU\u001b[0m/\n",
            " \u001b[01;34mtemp\u001b[0m/\n",
            " teste.txt.gdoc\n",
            " \u001b[01;34mtreinamento\u001b[0m/\n",
            " Untitled0.ipynb\n",
            "'Untitled document.gdoc'\n",
            " ViagemBuzios.gsheet\n",
            " \u001b[01;34mviagens\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mv coordenadas_lixoes*.tif curso_analise_imagens_satelite/FinalLixoes/Data/train/lixao"
      ],
      "metadata": {
        "id": "iBDlvASky3VR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owqPjUm_jBpO"
      },
      "source": [
        "## Declaring functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrQXbn9SjxoY"
      },
      "source": [
        "global n_classes\n",
        "\n",
        "def custom_sparse_categorical_accuracy(y_true,y_pred):\n",
        "  sh=tf.shape(y_pred)\n",
        "  y_pred = tf.reshape(y_pred, [sh[0]*sh[1],n_classes])   # adjust the last argument for the number of classes \n",
        "  y_true = tf.reshape(y_true, [sh[0]*sh[1],1])\n",
        "  \n",
        "  return tf.keras.metrics.sparse_categorical_accuracy(y_true,y_pred)\n",
        "\n",
        "def custom_sparse_categorical_crossentropy(y_true,y_pred):\n",
        "  sh=tf.shape(y_pred)\n",
        "  y_pred = tf.reshape(y_pred, [sh[0]*sh[1],n_classes]) # adjust the last argument for the number of classes \n",
        "  y_true = tf.reshape(y_true, [sh[0]*sh[1],1])\n",
        " \n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(y_true,y_pred)\n",
        "\n",
        "def count_images(path_to_samples):\n",
        "  n_images = 0\n",
        "  print(path_to_samples)\n",
        "  minpf = None\n",
        "  cd = 0\n",
        "  for label_num, subdir in enumerate(next(os.walk(path_to_samples))[1]):\n",
        "    x = len(os.listdir(path_to_samples + subdir))\n",
        "    print(\"dir:\",path_to_samples + subdir,x)\n",
        "    n_images += x\n",
        "    cd += 1\n",
        "    if minpf is None or x<minpf:\n",
        "      minpf=x\n",
        "\n",
        "  return n_images,minpf,cd\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_as_training_data(path_to_samples,experiment_type):\n",
        "\n",
        "  if experiment_type==0:\n",
        "    image_width = MINE_IMAGE_WIDTH\n",
        "    image_height = MINE_IMAGE_HEIGHT\n",
        "  else:\n",
        "    image_width = ORE_IMAGE_WIDTH\n",
        "    image_height = ORE_IMAGE_HEIGHT\n",
        "\n",
        "  Nbands = N_BANDS\n",
        "\n",
        "  # this script assumes that Sentinel 2 12 spectral bands images are being used\n",
        "  # opt_string = '-outsize {} {} -b 1 -b 2 -b 3 -b 4 -b 5 -b 6 -b 7 -b 8 -b 9 -b 10 -b 11 -b 12'.format(image_height, image_width)\n",
        "  opt_string = '-outsize {} {} -b 1 -b 2 -b 3 -b 4 -b 5 -b 6 -b 7'.format(image_height, image_width)\n",
        "  qtd_images,min_per_folder,dirs = count_images(path_to_samples)\n",
        "\n",
        "  # loads the same number of images from each folder\n",
        "  print(\"Will load {} images from each folder.\".format(min_per_folder))\n",
        "  n_images = min_per_folder*dirs\n",
        "\n",
        "  image = np.zeros((n_images, image_width, image_height, Nbands), dtype=np.float32)\n",
        "  label = -1 * np.ones((n_images,), dtype=np.int64)\n",
        "  char_labels = []\n",
        "\n",
        "  image_counter = 0\n",
        "  for label_num, subdir in enumerate(next(os.walk(path_to_samples))[1]):\n",
        "    char_labels.append(subdir)\n",
        "\n",
        "    print(\"Loading images for class\",path_to_samples + subdir)\n",
        "\n",
        "    files = np.array(os.listdir(path_to_samples + subdir))\n",
        "    permutation = np.random.permutation(len(files))\n",
        "    files = files[permutation]\n",
        "    count_this_folder = 0\n",
        "    for file in files:\n",
        "        img_dataset = gdal.Translate(NamedTemporaryFile(delete=False).name,\n",
        "                                     gdal.Open(path_to_samples + subdir + '/' + file, gdal.GA_ReadOnly),\n",
        "                                     options=opt_string)\n",
        "\n",
        "        img_array = np.transpose(np.array(img_dataset.ReadAsArray(),dtype = 'float32'), axes=(1, 2, 0))\n",
        "\n",
        "        image[image_counter, :, :, :] = img_array[:,:,0:Nbands]\n",
        "        label[image_counter] = label_num\n",
        "        image_counter += 1\n",
        "        count_this_folder +=1\n",
        "        if count_this_folder>=min_per_folder:   # keeping the same number of samples for all classes\n",
        "          break\n",
        "\n",
        "  return n_images,image,label,char_labels,image_width,image_height,Nbands,experiment_type\n",
        "    "
      ],
      "metadata": {
        "id": "HoPn0m5TxyGT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_training_data_to_file(path_to_model,image,label,charlabel,image_width,image_height,nmean,nmax,Nbands,n_classes,experiment_type):\n",
        "  print(\"Saving data:\",charlabel,path_to_model)\n",
        "  np.savez(path_to_model + 'trainingData',\n",
        "             images=image,\n",
        "             labels=label,\n",
        "             charlabels = charlabel)\n",
        "  \n",
        "  np.savez(path_to_model + 'modelMetaData',\n",
        "             metadata =np.array([image_width,image_height,Nbands,n_classes,charlabel,experiment_type])) \n",
        "\n",
        "  save_normal_std(path_to_model,nmean,nmax)\n",
        "  \n",
        "def load_training_data_from_file(path_to_model):\n",
        "  npzfile = np.load(path_to_model + 'trainingData.npz')\n",
        "  image = npzfile['images']\n",
        "  label = npzfile['labels']\n",
        "  charlabel = npzfile['charlabels']\n",
        " \n",
        "  n_images = image.shape[0]\n",
        "  metadata = np.load(path_to_model + 'modelMetaData.npz',allow_pickle=True)\n",
        "  image_width,image_height,n_bands,n_classes,char_labels,experiment_type = metadata['metadata'] \n",
        "\n",
        "  # training data was saved already normalized\n",
        "  nmean,nmax = load_normal_std(path_to_model)\n",
        "\n",
        "  return image,label,charlabel,n_images,image_width,image_height,n_bands,nmean,nmax,experiment_type\n"
      ],
      "metadata": {
        "id": "-vP8-Apux_7w"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_data(path_to_model,image,Nbands,nmean,nmax,save = True,loadfrom = None):\n",
        "  \n",
        "  given = True\n",
        "  if nmean is None:\n",
        "    if loadfrom is not None:\n",
        "      nmean,nmax = load_normal_std(loadfrom)\n",
        "    else:\n",
        "      nmean = np.zeros(Nbands)\n",
        "      nmax = np.zeros(Nbands)\n",
        "      given = False\n",
        "\n",
        "  # Normalize pixel values to be between 0 and 1\n",
        "  for i in range(Nbands):\n",
        "    if not given:\n",
        "      nmean[i] = np.mean(image[:,:,:,i])\n",
        "      nmax[i] = np.std(image[:,:,:,i])\n",
        "    image[:,:,:,i]=image[:,:,:,i]-nmean[i]\n",
        "    image[:,:,:,i]=image[:,:,:,i]/nmax[i]\n",
        "\n",
        "  if save:\n",
        "    save_normal_std(path_to_model,nmean,nmax)\n",
        "\n",
        "  return image,nmean,nmax\n",
        "\n",
        "def save_normal_std(path_to_model,nmean,nmax):\n",
        "  np.savez(path_to_model + 'trainingDataAvgStd',\n",
        "             median=nmean,\n",
        "             std=nmax)\n",
        "  \n",
        "def load_normal_std(path_to_model):\n",
        "  npzfile = np.load(path_to_model + 'trainingDataAvgStd.npz',)\n",
        "  nmean = npzfile['median']\n",
        "  nmax = npzfile['std']\n",
        "\n",
        "  return nmean,nmax\n",
        "\n"
      ],
      "metadata": {
        "id": "eeVWd7FmyKTI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_label_to_one_hot(labels, n_classes):\n",
        "    labels_arr = np.atleast_1d(labels)  # in case labels is a scalar\n",
        "    one_hot_labels = np.zeros((labels_arr.shape[0], n_classes), dtype = np.int)\n",
        "    one_hot_labels[np.arange(labels_arr.shape[0]), labels_arr] = 1\n",
        "    return one_hot_labels\n",
        "\n",
        "def split_training_test_sets(image,label,split_ratio):\n",
        "  # split_ratio defines the training share of the data. Must be between 0 and 1.\n",
        "\n",
        "  n_images,image_width,image_height,Nbands = image.shape\n",
        "\n",
        "  permutation = np.random.permutation(n_images)\n",
        "  image = image[permutation]\n",
        "  label = label[permutation]\n",
        "\n",
        "  Ntrain = int(n_images*split_ratio)\n",
        "  max_Ntrain = n_images\n",
        "\n",
        "  ls = list(label.shape)\n",
        "\n",
        "  ls[0]=Ntrain\n",
        "  train_images = np.zeros((Ntrain,image_width,image_height,Nbands))\n",
        "  train_labels = np.zeros(tuple(ls)) # (Ntrain,))\n",
        "\n",
        "  ls[0]=max_Ntrain - Ntrain\n",
        "  test_images = np.zeros((max_Ntrain - Ntrain,image_width,image_height,Nbands))\n",
        "  test_labels = np.zeros(tuple(ls)) #(max_Ntrain - Ntrain,))\n",
        "\n",
        "  train_images[:Ntrain] = image[:Ntrain] \n",
        "  train_labels[:Ntrain] = label[:Ntrain] \n",
        "\n",
        "  test_images[:] = image[Ntrain:]\n",
        "  test_labels[:] = label[Ntrain:]\n",
        "\n",
        "  return train_images,train_labels,test_images,test_labels\n"
      ],
      "metadata": {
        "id": "AKGz_nfnyQ7Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras"
      ],
      "metadata": {
        "id": "42X9HCMB4f_b"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_FCN_discovery(image_width,image_height,nc,Nbands):\n",
        "\n",
        "  global n_classes\n",
        "  n_classes = nc\n",
        "  kx = math.ceil((image_width-32)/27)  \n",
        "  ky = math.ceil((image_height-32)/27)  \n",
        "  # define the Fully Convolution Neural Network\n",
        "\n",
        "  data_augmentation = keras.Sequential(\n",
        "      [\n",
        "          layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "          layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "          layers.experimental.preprocessing.RandomZoom(0.2),\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  input=layers.Input(shape=(None, None, Nbands))\n",
        "  x=data_augmentation(input)\n",
        "  x0=layers.Conv2D(32, (3, 3), (1,1), activation='relu', padding='SAME')(x)\n",
        "  x1=layers.MaxPooling2D((3, 3))(x0)\n",
        "  x2=layers.Conv2D(64, (3, 3), (1,1), activation='relu', padding='SAME')(x1)\n",
        "  x3=layers.MaxPooling2D((3, 3))(x2)\n",
        "  x4=layers.Conv2D(64, (3, 3), (1,1), activation='relu', padding='SAME')(x3)\n",
        "  x5=layers.MaxPooling2D((3,3))(x4)\n",
        "  x6=layers.Conv2D(64, (kx,ky), (1,1), activation = 'relu')(x5)  # the convolution kernel size must be adapted for image size \n",
        "  x7=layers.Dropout(0.5)(x6)\n",
        "  x8=layers.Conv2D(n_classes, (1,1), (1,1), padding='VALID', activation='softmax')(x7)\n",
        "  model=models.Model(input,x8)\n",
        "  model.summary()\n",
        "\n",
        "  model.compile(optimizer='adam',\n",
        "              loss=custom_sparse_categorical_crossentropy,\n",
        "              metrics=[custom_sparse_categorical_accuracy])\n",
        "             \n",
        "  return model\n"
      ],
      "metadata": {
        "id": "lEv94TNnyUm5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_cross_entropy_with_logits(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred))\n",
        "\n",
        "\"\"\"\n",
        "def define_FCN_oretype(image_width,image_height,nc,Nbands):\n",
        "\n",
        "  global n_classes\n",
        "  n_classes = nc\n",
        "  \n",
        "  input_shape=(image_width, image_height, Nbands)\n",
        "  print(input_shape)\n",
        "  cnn_model = tf.keras.models.Sequential()\n",
        "  cnn_model.add(tf.keras.layers.InputLayer(input_shape=input_shape))\n",
        "  cnn_model.add(tf.keras.layers.Conv2D(32, (2, 2), strides=1, padding='same'))\n",
        "  cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "  cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "  cnn_model.add(tf.keras.layers.Conv2D(64, (2, 2), strides=1, padding='same'))\n",
        "  cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "  cnn_model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "  cnn_model.add(tf.keras.layers.Flatten())\n",
        "  cnn_model.add(tf.keras.layers.Dense(1024))\n",
        "  cnn_model.add(tf.keras.layers.Activation('relu'))\n",
        "  cnn_model.add(tf.keras.layers.Dropout(0.5))\n",
        "  cnn_model.add(tf.keras.layers.Dense(n_classes, activation='softmax'))\n",
        "  cnn_model.summary()\n",
        "\n",
        "  adam = tf.keras.optimizers.Adam(lr=1e-4)\n",
        "\n",
        "  cnn_model.compile(optimizer=adam, loss=tf_cross_entropy_with_logits, metrics=['accuracy'])\n",
        "\n",
        "  return cnn_model\n",
        "\"\"\"             \n",
        "\n",
        "def fit_model(path_to_model,model, train_images, train_labels, epochs = 20, batchsize = 16, plot_accuracy = True, save_model = True):\n",
        "\n",
        "  # Create a callback that saves the model's weights\n",
        "  if save_model:\n",
        "\n",
        "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath= path_to_model + 'trained_model/weights.ckpt',\n",
        "                                                    save_weights_only=True,\n",
        "                                                    verbose=1)\n",
        "\n",
        "    fit = model.fit(train_images, train_labels, epochs=epochs, batch_size=batchsize,callbacks=[cp_callback])\n",
        "  else:\n",
        "    fit = model.fit(train_images, train_labels, epochs=epochs, batch_size=batchsize)\n",
        "\n",
        "  accuracy = fit.history[list(fit.history.keys())[-1]]\n",
        "\n",
        "  if plot_accuracy:\n",
        "    plt.plot(range(epochs), accuracy, 'r--', label='train')\n",
        "    plt.title('Training accuracy')\n",
        "    plt.show()\n",
        "\n",
        "  return model,accuracy\n",
        "\n",
        "def custom_fit(path_to_model,model_type, image, label,image_width,image_height,n_classes,Nbands,batchsize = 16, plot_accuracy = True, save_model = True):\n",
        "\n",
        "  print(\"How many folds for cross validation? \")\n",
        "  try:\n",
        "    n_cv_runs=int(input('Input:'))\n",
        "  except ValueError:\n",
        "    print(\"Not a number\")\n",
        "\n",
        "  print(\"How many epochs at each fold ? \")\n",
        "  try:\n",
        "    n_epoch=int(input('Input:'))\n",
        "  except ValueError:\n",
        "    print(\"Not a number\")\n",
        "\n",
        "  bestaccuracy = -1\n",
        "  accuracyTrain = np.zeros((n_cv_runs,n_epoch))\n",
        "  accuracyTest = np.zeros((n_cv_runs,n_epoch))\n",
        "  f1Score= np.zeros((n_cv_runs,n_epoch))\n",
        "  confusionMatrix = np.zeros((n_cv_runs,n_epoch,n_classes,n_classes))\n",
        "  precision = np.zeros(n_cv_runs)\n",
        "  recall = np.zeros(n_cv_runs)\n",
        "  roc_auc = np.zeros(n_cv_runs)\n",
        "  matthews = np.zeros(n_cv_runs)\n",
        "  kappa = np.zeros(n_cv_runs)\n",
        "\n",
        "  models = []\n",
        "  # define model and compile\n",
        "  if model_type==0:\n",
        "    model = define_FCN_discovery(image_width,image_height,n_classes,Nbands)\n",
        "  else:\n",
        "    model = define_FCN_oretype(image_width,image_height,n_classes,Nbands)\n",
        "  # save initial state\n",
        "  weights = model.get_weights() \n",
        "\n",
        "  # shuffle image set\n",
        "  permutation = np.random.permutation(len(image))\n",
        "  image = image[permutation]\n",
        "  label = label[permutation]\n",
        "\n",
        "  kf = KFold(n_splits=n_cv_runs)\n",
        " \n",
        "  cv_run = 0\n",
        "  for train_idx, val_idx in kf.split(image,label):\n",
        "    train_images = image[train_idx]\n",
        "    train_labels = label[train_idx]\n",
        "    test_images = image[val_idx]\n",
        "    test_labels = label[val_idx]\n",
        "\n",
        "    print('\\tfold %d...' % cv_run)\n",
        "\n",
        "    for epoch in range(n_epoch):\n",
        "      print('\\tepoch %d...' % epoch)\n",
        "\n",
        "      fit = model.fit(train_images, train_labels, epochs=1, batch_size=batchsize)\n",
        "      \n",
        "      accuracyTrain[cv_run,epoch] = fit.history[list(fit.history.keys())[-1]][-1]\n",
        "\n",
        "      # testing\n",
        "      prediction = model.predict(test_images)\n",
        "      predics = np.zeros(len(prediction))\n",
        "      if model_type==0:\n",
        "        predics = prediction[:,0,0,:].argmax(axis=1) \n",
        "        tl = test_labels \n",
        "      else:\n",
        "        predics = prediction[:,:].argmax(axis=1) \n",
        "        tl = test_labels.argmax(axis=1)\n",
        "\n",
        "      # metrics\n",
        "      accuracyTest[cv_run,epoch] = accuracy_score(tl, predics)\n",
        "      if n_classes == 2:\n",
        "        f1Score[cv_run,epoch]  = f1_score(tl, predics)\n",
        "      confusionMatrix[cv_run,epoch] = confusion_matrix(tl, predics)\n",
        "      \n",
        "    print(\"Final metrics on fold\",cv_run,\": accuracy on training:\",accuracyTrain[cv_run,-1],\"accuracy on testing:\",accuracyTest[cv_run,-1])\n",
        "    if n_classes == 2:\n",
        "      print(\"F1 score:\",f1Score[cv_run,-1])\n",
        "      show_metrics_sklearn(tl, predics)\n",
        "    print(\"Confusion matrix:\",confusionMatrix[cv_run,-1])\n",
        "    \n",
        "    if n_classes == 2:\n",
        "      precision[cv_run] = metrics.precision_score(tl, predics)\n",
        "      recall[cv_run] = metrics.recall_score(tl, predics)\n",
        "      roc_auc[cv_run] = metrics.roc_auc_score(tl, predics)\n",
        "\n",
        "    matthews[cv_run] = metrics.matthews_corrcoef(tl, predics)\n",
        "    kappa[cv_run] = metrics.cohen_kappa_score(tl, predics)\n",
        "\n",
        "    plt.plot(range(n_epoch), accuracyTrain[cv_run], 'r--', label='train')\n",
        "    plt.plot(range(n_epoch), accuracyTest[cv_run], 'b-', label='test')\n",
        "    plt.title('Accuracy {} run'.format(cv_run))\n",
        "    plt.savefig(path_to_model + 'accuracy_results_{}.pdf'.format(cv_run))\n",
        "    plt.show()\n",
        "\n",
        "    model.save(path_to_model + 'trained_model/weights_{}_run.ckpt'.format(cv_run))\n",
        " \n",
        "    np.savez(path_to_model + 'results_{}_run'.format(cv_run),\n",
        "              train_accuracy=accuracyTrain[cv_run],\n",
        "              f1Score=f1Score[cv_run],\n",
        "              test_accuracy=accuracyTest[cv_run],\n",
        "              confusionMatrix=confusionMatrix[cv_run],\n",
        "              precision = precision[cv_run],\n",
        "              recall = recall[cv_run],\n",
        "              roc_auc = roc_auc[cv_run],\n",
        "              matthews = matthews[cv_run],\n",
        "              kappa = kappa[cv_run])\n",
        "    \n",
        "    # return model to initial state\n",
        "    model.set_weights(weights)\n",
        "    del train_images,train_labels,test_images,test_labels\n",
        "    gc.collect()\n",
        "    cv_run+=1\n",
        "\n",
        "  print(\"Summary:\")\n",
        "  for cv_run in range(n_cv_runs):\n",
        "    plt.plot(range(n_epoch), accuracyTrain[cv_run], 'r--', label='train{}_run'.format(cv_run))\n",
        "    plt.plot(range(n_epoch), accuracyTest[cv_run], 'b-', label='test{}_run'.format(cv_run))\n",
        "    print(\"Fold\",cv_run,\": accuracy on training:\",accuracyTrain[cv_run,-1],\"accuracy on testing:\",accuracyTest[cv_run,-1],\"F1 score:\",f1Score[cv_run,-1])\n",
        "\n",
        "  plt.title('Accuracy final')\n",
        "  plt.savefig(path_to_model + 'accuracy_results_final.pdf')\n",
        "  plt.show()\n",
        "\n",
        "  print(\"Which model do you want to use?\")\n",
        "  try:\n",
        "    id=int(input('Input:'))\n",
        "  except ValueError:\n",
        "    print(\"Not a number\")\n",
        "\n",
        "  np.savez(path_to_model + 'results_final',\n",
        "              train_accuracy=accuracyTrain,\n",
        "              f1Score=f1Score,\n",
        "              test_accuracy=accuracyTest,\n",
        "              confusionMatrix=confusionMatrix,\n",
        "              precision = precision,\n",
        "              recall = recall,\n",
        "              roc_auc = roc_auc,\n",
        "              matthews = matthews,\n",
        "              kappa = kappa,\n",
        "              chosen = id)\n",
        "\n",
        "  # load the chosen model from disk\n",
        "  model.load_weights(path_to_model + 'trained_model/weights_{}_run.ckpt/variables/variables'.format(id))\n",
        "\n",
        "  return model\n",
        "\n",
        "def show_model_metrics(path_to_model):\n",
        "  npzfile = np.load(path_to_model + 'results_final.npz')\n",
        "          \n",
        "  train_accuracy = npzfile['train_accuracy']\n",
        "  f1Score = npzfile['f1Score']\n",
        "  test_accuracy = npzfile['test_accuracy']\n",
        "  confusionMatrix = npzfile['confusionMatrix']\n",
        "  precision = npzfile['precision']\n",
        "  recall = npzfile['recall']\n",
        "  roc_auc = npzfile['roc_auc']\n",
        "  matthews = npzfile['matthews']\n",
        "  kappa = npzfile['kappa']\n",
        "  chosen = npzfile['chosen']\n",
        " \n",
        "  print(\"train acc:\",train_accuracy[:,-1])\n",
        "  print(\"train acc:\",test_accuracy[:,-1])\n",
        "  folds = train_accuracy.shape[0]\n",
        "  epochs = train_accuracy.shape[1]\n",
        "  print(\"Average metrics for the last epoch of {} folds:\".format(folds))\n",
        "  print('train_accuracy:',train_accuracy[:,-1].mean())\n",
        "  print('train_accuracy std:',train_accuracy[:,-1].std())\n",
        "  print('train_accuracy range:',train_accuracy[:,-1].ptp())\n",
        "  print('train_accuracy % range:',train_accuracy[:,-1].ptp()/2*train_accuracy[:,-1].mean())\n",
        "  print('test_accuracy:',test_accuracy[:,-1].mean()) \n",
        "  print('test_accuracy std:',test_accuracy[:,-1].std()) \n",
        "  print('test_accuracy range:',test_accuracy[:,-1].ptp()) \n",
        "  print('test_accuracy % range:',test_accuracy[:,-1].ptp()/2*test_accuracy[:,-1].mean()) \n",
        "  print('f1Score:',f1Score[:,-1].mean())  \n",
        "  print('confusionMatrix:',confusionMatrix[:,-1,:,:].mean(axis=0))\n",
        "  print('precision:',precision.mean())\n",
        "  print('recall:',recall.mean())\n",
        "  print('roc_auc:',roc_auc.mean())\n",
        "  print('matthews:',matthews.mean())\n",
        "  print('kappa:',kappa.mean())\n",
        "\n",
        "\n",
        "  print(\"Metrics for the chosen model:\")\n",
        "  print('train_accuracy:',train_accuracy[chosen,-1])\n",
        "  print('test_accuracy:',test_accuracy[chosen,-1]) \n",
        "  print('f1Score:',f1Score[chosen,-1])  \n",
        "  print('confusionMatrix:',confusionMatrix[chosen,-1,:,:])\n",
        "  print('precision:',precision[chosen])\n",
        "  print('recall:',recall[chosen])\n",
        "  print('roc_auc:',roc_auc[chosen])\n",
        "  print('matthews:',matthews[chosen])\n",
        "  print('kappa:',kappa[chosen])\n",
        "\n",
        "  IFrame(path_to_model + 'accuracy_results_final.pdf', width=600, height=300)\n",
        "\n",
        "  \n",
        "def load_weights(path_to_model,model):\n",
        "    \n",
        "  if os.path.exists(path_to_model + 'trained_model/checkpoint'): \n",
        "    print(\"Single trained model found. Do you want to load it? (1:yes/0:no)\" )\n",
        "    try:\n",
        "      resp=int(input('Input:'))\n",
        "    except ValueError:\n",
        "      print(\"Not a number\")\n",
        "    if resp==1:\n",
        "      model.load_weights(path_to_model + 'trained_model/weights.ckpt')\n",
        "      return model\n",
        "\n",
        "  if os.path.exists(path_to_model + 'results_final.npz'): # check if there is k-fold saved models available\n",
        "    results = np.load(path_to_model + 'results_final.npz')\n",
        "    print(\"K-fold cross validation trained models found:\")\n",
        "    for i in range(0,len(results['train_accuracy'])):\n",
        "      print('Model {}: train accuracy = {} test accuracy = {} f1 score = {}'.format(i,results['train_accuracy'][i,-1],results['test_accuracy'][i,-1],results['f1Score'][i,-1]))\n",
        "    print(\"Which one do you want to load?\" )\n",
        "    try:\n",
        "      resp=int(input('Input:'))\n",
        "    except ValueError:\n",
        "      print(\"Not a number\")\n",
        "    if resp < 0 or resp>len(results['train_accuracy']):\n",
        "      raise Exception(\"Invalid model choice\")\n",
        "\n",
        "    model.load_weights(path_to_model + 'trained_model/weights_{}_run.ckpt/variables/variables'.format(resp))\n",
        "\n",
        "    return model\n",
        "\n",
        "  raise Exception(\"No model loaded\")\n",
        "\n",
        "def load_model(path_to_model):\n",
        "  \n",
        "  metadata = np.load(path_to_model + 'modelMetaData.npz',allow_pickle=True)\n",
        "  image_width,image_height,n_bands,n_classes,char_labels,experiment_type = metadata['metadata'] \n",
        "\n",
        "  if experiment_type==0:  # dams discovery  \n",
        "    model = define_FCN_discovery(image_width,image_height,n_classes,n_bands)\n",
        "  else:\n",
        "    model = define_FCN_oretype(image_width,image_height,n_classes,n_bands)\n",
        "\n",
        "  model= load_weights(path_to_model,model)\n",
        "  nmean,nmax = load_normal_std(path_to_model)\n",
        "  \n",
        "  return model,image_width,image_height,n_bands,n_classes,nmean,nmax,char_labels,experiment_type\n",
        "\n",
        "def train_test(path_to_model,model_type,image,label,image_width,image_height,n_classes,Nbands,clearMemory = True):\n",
        "\n",
        "  if model_type==1:  # not a FCN\n",
        "    label = convert_label_to_one_hot(label,n_classes)\n",
        "\n",
        "  print(\"Training and testing (0.8 - 0.2 split):1, training for predicting (no split, no testing): 2 or customized learning : 3 ? ((1/2/3))\")\n",
        "  try:\n",
        "    r=int(input('Input:'))\n",
        "  except ValueError:\n",
        "    print(\"Not a number\")\n",
        "\n",
        "  if r==3:\n",
        "    model = custom_fit(path_to_model,model_type, image,label,image_width,image_height,n_classes,Nbands)\n",
        "  else:\n",
        "    if model_type==0:\n",
        "      model = define_FCN_discovery(image_width,image_height,n_classes,Nbands)\n",
        "    else:\n",
        "      model = define_FCN_oretype(image_width,image_height,n_classes,Nbands)\n",
        "\n",
        "    if r == 1:\n",
        "      train_images,train_labels,test_images,test_labels = split_training_test_sets(image,label,0.8)\n",
        "      model,accuracy = fit_model(path_to_model,model, train_images, train_labels)\n",
        "      # testing\n",
        "      print(\"Testing:\")\n",
        "      prediction = model.predict(test_images)\n",
        "      \n",
        "      predics = np.zeros(len(prediction))\n",
        "      if model_type==0:\n",
        "        predics = prediction[:,0,0,:].argmax(axis=1) \n",
        "      else:\n",
        "        predics = prediction[:,:].argmax(axis=1)\n",
        "        test_labels = test_labels[:,:].argmax(axis=1)\n",
        "\n",
        "      print(confusion_matrix(test_labels, predics))\n",
        "      if n_classes==2:\n",
        "        show_metrics_sklearn(test_labels, predics)\n",
        "      \n",
        "      del test_images\n",
        "      del test_labels\n",
        "      del train_images\n",
        "      del train_labels\n",
        "    else: \n",
        "      model,accuracy = fit_model(path_to_model,model, image,label)\n",
        "      print(\"Final accuracy:\",accuracy[-1])\n",
        "\n",
        "  if clearMemory:\n",
        "    del image\n",
        "    del label\n",
        "    \n",
        "  gc.collect()\n",
        "\n",
        "  return model\n",
        "\n",
        "def predict(main_model,ore_classifier,n_images,main_labels,ore_labels,image_width,image_height,nmean,nmax,Nbands,slicing = 1):\n",
        "\n",
        "  countImages = 0\n",
        "  \n",
        "  print(\"Choose target class: \")\n",
        "  for i in range(0, len(main_labels)):\n",
        "    print(i,\":\",main_labels[i])\n",
        "  try:\n",
        "    save=int(input('Input:'))\n",
        "  except ValueError:\n",
        "    print(\"Not a number\")\n",
        "\n",
        "  print(\"Choose the no ore class: \")\n",
        "  for i in range(0, len(ore_labels)):\n",
        "    print(i,\":\",ore_labels[i])\n",
        "  try:\n",
        "    noore=int(input('Input:'))\n",
        "  except ValueError:\n",
        "    print(\"Not a number\")\n",
        "\n",
        "  kmlFull=simplekml.Kml()\n",
        "  resultsFull = [] \n",
        "\n",
        "  # process each image on the predict folder\n",
        "  for file in os.listdir(path_to_predict_images):\n",
        "    print(file)\n",
        "    starthere = len(resultsFull)\n",
        "\n",
        "    testimage = gdal.Translate(NamedTemporaryFile(delete=False).name,\n",
        "                                  gdal.Open(path_to_predict_images + file, gdal.GA_ReadOnly),\n",
        "                                  options = '-b 1 -b 2 -b 3 -b 4 -b 5 -b 6 -b 7 -b 8 -b 9 -b 10 -b 11 -b 12')\n",
        "\n",
        "    testimage = np.transpose(np.array(testimage.ReadAsArray(),dtype = 'float32'), axes=(1, 2, 0))\n",
        "    testimage = testimage[:,:,0:Nbands]\n",
        "    for i in range(Nbands):\n",
        "      testimage[:,:,i]=testimage[:,:,i]-nmean[i]\n",
        "      testimage[:,:,i]=testimage[:,:,i]/nmax[i]\n",
        "\n",
        "    # slice the image for prediction if it is to too large for colab. \n",
        "    # Divides the image and regular 2D patches\n",
        "    # Adapt the \"slicing\" for your size of image and colab memory availability:  \n",
        "    \n",
        "    sh = np.array(testimage.shape)\n",
        "    sh[0] = sh[0]//slicing\n",
        "    sh[1] = sh[1]//slicing\n",
        "    prediction = None\n",
        "    for i in range(0,slicing):  \n",
        "      for j in range(0,slicing):\n",
        "        patch = np.expand_dims(testimage[sh[0]*i:sh[0]*(i+1),sh[1]*j:sh[1]*(j+1),:],axis=0) \n",
        "        # compute prediction of the FCN\n",
        "        if prediction is None:\n",
        "          prediction = main_model.predict(patch)\n",
        "        else:\n",
        "          prediction = np.append(prediction,main_model.predict(patch), axis=0)\n",
        "      \n",
        "    # locate the predictions on the patches\n",
        "\n",
        "    shp = prediction.shape\n",
        "    results = []\n",
        "    lpatches = []\n",
        "    kml=simplekml.Kml()\n",
        "    count_positives = 0\n",
        "\n",
        "    with rasterio.open(path_to_predict_images+file) as map_layer:\n",
        "      for i in range(0,slicing):  \n",
        "        for j in range(0,slicing): \n",
        " \n",
        "          pos = i*slicing+j\n",
        "          \n",
        "          pred_classes = tf.greater(prediction[pos,:,:,save],0.6) # considering a 60% probability threshold to accept a positive\n",
        "          shpc = tf.shape(pred_classes)\n",
        "\n",
        "          inds = tf.where(pred_classes==True)\n",
        "\n",
        "          # Upsampling to original image dimensions.\n",
        "          \n",
        "          xloc = i*sh[0]+np.array(tf.gather(np.linspace(image_width/2.0,sh[0]-image_width/2.0,num=shpc[0]),inds[:,0]))\n",
        "          yloc = j*sh[1]+np.array(tf.gather(np.linspace(image_height/2.0,sh[1]-image_height/2.0,num=shpc[1]),inds[:,1]))\n",
        "          predloc = prediction[pos,inds[:,0],inds[:,1],save]\n",
        "\n",
        "          for k in range(0,len(xloc)):\n",
        "            pixels2coords = map_layer.xy(xloc[k] , yloc[k]) \n",
        "\n",
        "            r = np.ones(4)*-1      \n",
        "            \n",
        "            r[0] = pixels2coords[0]\n",
        "            r[1] = pixels2coords[1]\n",
        "            r[2] = predloc[k]\n",
        "\n",
        "            # prepare ore type prediction\n",
        "            if ore_classifier is not None: \n",
        "              #lx = int(round(xloc[k]-(ORE_IMAGE_WIDTH)))\n",
        "              lx = int(round(xloc[k]-(ORE_IMAGE_WIDTH//2)))\n",
        "              if lx<0:\n",
        "                lx=0\n",
        "              #ly = int(round(yloc[k]-(ORE_IMAGE_HEIGHT)))\n",
        "              ly = int(round(yloc[k]-(ORE_IMAGE_HEIGHT//2)))\n",
        "              if ly<0:\n",
        "                ly=0\n",
        "              hx = lx+ORE_IMAGE_WIDTH\n",
        "              if hx>testimage.shape[0]:\n",
        "                hx=testimage.shape[0]\n",
        "              \n",
        "              hy = ly+ORE_IMAGE_HEIGHT\n",
        "              if hy>testimage.shape[1]:\n",
        "                hy=testimage.shape[1]\n",
        "              \n",
        "              lpatch = np.expand_dims(testimage[lx:hx,ly:hy,:],axis=0)  \n",
        "              ore_prediction = ore_classifier.predict(lpatch)\n",
        "              \n",
        "              # ore type winner\n",
        "              r[-1] = ore_prediction[0,:].argmax()\n",
        "              tag = ore_labels[r[-1].astype(int)]\n",
        "            else:\n",
        "              tag = main_labels[save]\n",
        "              \n",
        "            if r[-1] != noore: # if no ore was detected discard point\n",
        "              kml.newpoint(name=tag, coords=[(pixels2coords[0],pixels2coords[1])])\n",
        "              kmlFull.newpoint(name=tag, coords=[(pixels2coords[0],pixels2coords[1])])\n",
        "\n",
        "              results.append(r)\n",
        "              resultsFull.append(r)\n",
        "\n",
        "    # save prediction files to the results folder\n",
        "    np.savetxt(path_to_results + 'prediction_{}.csv'.format(file), results, delimiter=\",\")\n",
        "    kml.save(path_to_results + 'predictions_{}.kml'.format(file))  \n",
        "\n",
        "    countImages+=1\n",
        "\n",
        "    del testimage\n",
        "    del patch\n",
        "    del prediction\n",
        "    del results\n",
        "    del kml\n",
        "    gc.collect()\n",
        "\n",
        "    if n_images != -1 and countImages>=n_images:\n",
        "      break\n",
        "\n",
        "  np.savetxt(path_to_results + 'prediction_{}.csv'.format(experiment_name), np.array(resultsFull), delimiter=\",\")\n",
        "  kmlFull.save(path_to_results + 'predictions_{}.kml'.format(experiment_name))  \n"
      ],
      "metadata": {
        "id": "puLlrHyUybch"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_workflow(path_to_model, path_to_images, model_type,main_model = None,nmean=None,nmax=None):\n",
        "\n",
        "  load = False\n",
        "  if os.path.exists(path_to_model + 'trained_model/checkpoint') or os.path.exists(path_to_model + 'results_final.npz'): # check if there is a saved model available\n",
        "    print(\"Saved model found. Do you want to load it? ((1:yes/0:no))\")\n",
        "    try:\n",
        "      r=int(input('Input:'))\n",
        "    except ValueError:\n",
        "      print(\"Not a number\")\n",
        "    if r == 1:\n",
        "      model,image_width,image_height,Nbands,n_classes,nmean,nmax,char_labels,experiment_type = load_model(path_to_model)\n",
        "      load = True\n",
        "\n",
        "  if not load:  # training new model\n",
        "    # load training data from images or from array\n",
        "    loaded = False\n",
        "    if os.path.exists(path_to_model + 'trainingData.npz'):\n",
        "      print(\"Saved training data found. Do you want to load it? ((1:yes/0:no))\")\n",
        "      try:\n",
        "        r=int(input('Input:'))\n",
        "      except ValueError:\n",
        "        print(\"Not a number\")\n",
        "      if r == 1:\n",
        "        # loading data from array already normalized\n",
        "        image,label,char_labels,n_images,image_width,image_height,Nbands,nmean,nmax,experiment_type = load_training_data_from_file(path_to_model)\n",
        "        loaded = True\n",
        "\n",
        "    if not loaded: # load data from images and normalize\n",
        "\n",
        "      n_images,image,label,char_labels,image_width,image_height,Nbands,experiment_type = load_images_as_training_data(path_to_images,model_type)\n",
        "      image,nmean,nmax =  normalize_data(path_to_model,image,Nbands,nmean,nmax)\n",
        "\n",
        "      print(\"Training data loaded from the image sets:\",n_images,char_labels)\n",
        "      print(\"Do you want to save images as arrays for faster future loading? ((1:yes/0:no))\")\n",
        "      try:\n",
        "        r=int(input('Input:'))\n",
        "      except ValueError:\n",
        "        print(\"Not a number\")\n",
        "      if r == 1:\n",
        "        save_training_data_to_file(path_to_model,image,label,char_labels,image_width,image_height,nmean,nmax,Nbands,len(char_labels),model_type) # saving normalized data\n",
        "\n",
        "    n_classes = len(char_labels)\n",
        "    print(\"Training data loaded:\",image_width,image_height,Nbands,n_classes,char_labels,np.isnan(np.sum(image)))\n",
        "    model = train_test(path_to_model, model_type,image,label,image_width,image_height,n_classes,Nbands)\n",
        "\n",
        "  return model,char_labels,image_width,image_height,nmean,nmax\n",
        "\n",
        "def show_metrics_sklearn(y_true, y_pred):\n",
        "  print(\"Precision: \", metrics.precision_score(y_true, y_pred))\n",
        "  print(\"Recall: \", metrics.recall_score(y_true, y_pred))\n",
        "  print(\"F1 score: \", metrics.f1_score(y_true, y_pred))\n",
        "  print(\"Mean accuracy: \", metrics.accuracy_score(y_true, y_pred))\n",
        "  print(\"AUC: \", metrics.roc_auc_score(y_true, y_pred))\n",
        "  print(\"mcc: \", metrics.matthews_corrcoef(y_true, y_pred))\n",
        "  print(\"kappa: \", metrics.cohen_kappa_score(y_true, y_pred))\n"
      ],
      "metadata": {
        "id": "HUqH6WKA0XFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3rFfcgdT9v-"
      },
      "source": [
        "## Main workflow:\n",
        "\n",
        "The following script was used to implement the method described on the paper \"Mining and Tailings Dam Detection In Satellite Imagery Using Deep Learning\". It trains a first CNN to identify surface mines on satellite images and a second CNN to classify the potential environment impact of the discovered mining spots. If trained models are detected on the \"Experiments\" folder the user can choose to load them instead of retrain new models. Trained or loaded models can then be used to discover and classify mines on large areas represented by satellite images contained  \"predict\" folder. Results are recorded on files made available on the \"Results\" folder."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load or train main experiment (mines an dams discovery)\n",
        "print(\"Discovery model:\")\n",
        "main_model,main_labels,image_width,image_height,nmean,nmax = model_workflow(path_to_output, path_to_images, 0)\n",
        "print(main_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SViXf3jL8mcv",
        "outputId": "9e3efc67-7679-4f75-cc11-bb68f7d02bc5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discovery model:\n",
            "Saved model found. Do you want to load it? ((1:yes/0:no))\n",
            "Input:0\n",
            "Saved training data found. Do you want to load it? ((1:yes/0:no))\n",
            "Input:0\n",
            "./curso_analise_imagens_satelite/FinalLixoes/Data//train/\n",
            "dir: ./curso_analise_imagens_satelite/FinalLixoes/Data//train/lixao 270\n",
            "dir: ./curso_analise_imagens_satelite/FinalLixoes/Data//train/naolixao 176\n",
            "Will load 176 images from each folder.\n",
            "Loading images for class ./curso_analise_imagens_satelite/FinalLixoes/Data//train/lixao\n",
            "Loading images for class ./curso_analise_imagens_satelite/FinalLixoes/Data//train/naolixao\n",
            "Training data loaded from the image sets: 352 ['lixao', 'naolixao']\n",
            "Do you want to save images as arrays for faster future loading? ((1:yes/0:no))\n",
            "Input:1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data: ['lixao', 'naolixao'] ./curso_analise_imagens_satelite/FinalLixoes/Experiments//\n",
            "Training data loaded: 40 40 7 2 ['lixao', 'naolixao'] True\n",
            "Training and testing (0.8 - 0.2 split):1, training for predicting (no split, no testing): 2 or customized learning : 3 ? ((1/2/3))\n",
            "Input:1\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, None, None, 7)]   0         \n",
            "                                                                 \n",
            " sequential_2 (Sequential)   (None, None, None, 7)     0         \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, None, None, 32)    2048      \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  (None, None, None, 32)   0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, None, None, 64)    18496     \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPoolin  (None, None, None, 64)   0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, None, None, 64)    36928     \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPoolin  (None, None, None, 64)   0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, None, None, 64)    4160      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, None, None, 64)    0         \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, None, None, 2)     130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61,762\n",
            "Trainable params: 61,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6933 - custom_sparse_categorical_accuracy: 0.4596\n",
            "Epoch 1: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 1s 11ms/step - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.4662\n",
            "Epoch 2/20\n",
            "16/18 [=========================>....] - ETA: 0s - loss: 0.6931 - custom_sparse_categorical_accuracy: 0.5352\n",
            "Epoch 2: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.6933 - custom_sparse_categorical_accuracy: 0.5089\n",
            "Epoch 3/20\n",
            "15/18 [========================>.....] - ETA: 0s - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.4958\n",
            "Epoch 3: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5089\n",
            "Epoch 4/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5037\n",
            "Epoch 4: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5089\n",
            "Epoch 5/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6931 - custom_sparse_categorical_accuracy: 0.5110\n",
            "Epoch 5: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.6931 - custom_sparse_categorical_accuracy: 0.5089\n",
            "Epoch 6/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5074\n",
            "Epoch 6: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5089\n",
            "Epoch 7/20\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.6931 - custom_sparse_categorical_accuracy: 0.5089\n",
            "Epoch 7: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 15ms/step - loss: 0.6931 - custom_sparse_categorical_accuracy: 0.5089\n",
            "Epoch 8/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6933 - custom_sparse_categorical_accuracy: 0.5037\n",
            "Epoch 8: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5089\n",
            "Epoch 9/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6929 - custom_sparse_categorical_accuracy: 0.5184\n",
            "Epoch 9: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.6931 - custom_sparse_categorical_accuracy: 0.5089\n",
            "Epoch 10/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5037\n",
            "Epoch 10: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6931 - custom_sparse_categorical_accuracy: 0.5089\n",
            "Epoch 11/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5037\n",
            "Epoch 11: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.6931 - custom_sparse_categorical_accuracy: 0.5089\n",
            "Epoch 12/20\n",
            "16/18 [=========================>....] - ETA: 0s - loss: 0.6930 - custom_sparse_categorical_accuracy: 0.5117\n",
            "Epoch 12: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.6930 - custom_sparse_categorical_accuracy: 0.5089\n",
            "Epoch 13/20\n",
            "15/18 [========================>.....] - ETA: 0s - loss: 0.6929 - custom_sparse_categorical_accuracy: 0.5125\n",
            "Epoch 13: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6930 - custom_sparse_categorical_accuracy: 0.5089\n",
            "Epoch 14/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6931 - custom_sparse_categorical_accuracy: 0.5074\n",
            "Epoch 14: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.6931 - custom_sparse_categorical_accuracy: 0.5089\n",
            "Epoch 15/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6929 - custom_sparse_categorical_accuracy: 0.5147\n",
            "Epoch 15: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.6930 - custom_sparse_categorical_accuracy: 0.5089\n",
            "Epoch 16/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6930 - custom_sparse_categorical_accuracy: 0.5110\n",
            "Epoch 16: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6930 - custom_sparse_categorical_accuracy: 0.5089\n",
            "Epoch 17/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6931 - custom_sparse_categorical_accuracy: 0.5074\n",
            "Epoch 17: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6931 - custom_sparse_categorical_accuracy: 0.5089\n",
            "Epoch 18/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6931 - custom_sparse_categorical_accuracy: 0.5074\n",
            "Epoch 18: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6930 - custom_sparse_categorical_accuracy: 0.5089\n",
            "Epoch 19/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5037\n",
            "Epoch 19: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.6930 - custom_sparse_categorical_accuracy: 0.5089\n",
            "Epoch 20/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6934 - custom_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 20: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5089\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing:\n",
            "[[33  0]\n",
            " [38  0]]\n",
            "Precision:  0.0\n",
            "Recall:  0.0\n",
            "F1 score:  0.0\n",
            "Mean accuracy:  0.4647887323943662\n",
            "AUC:  0.5\n",
            "mcc:  0.0\n",
            "kappa:  0.0\n",
            "['lixao', 'naolixao']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load or train main experiment (mines an dams discovery)\n",
        "print(\"Discovery model:\")\n",
        "main_model,main_labels,image_width,image_height,nmean,nmax = model_workflow(path_to_output, path_to_images, 0)\n",
        "print(main_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "id": "I5IbRXLe2ZQV",
        "outputId": "919ec3fa-31f5-407a-9967-cacee9e1198a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discovery model:\n",
            "Saved model found. Do you want to load it? ((1:yes/0:no))\n",
            "Input:0\n",
            "Saved training data found. Do you want to load it? ((1:yes/0:no))\n",
            "Input:0\n",
            "./curso_analise_imagens_satelite/FinalLixoes/Data//train/\n",
            "dir: ./curso_analise_imagens_satelite/FinalLixoes/Data//train/lixao 270\n",
            "dir: ./curso_analise_imagens_satelite/FinalLixoes/Data//train/naolixao 176\n",
            "Will load 176 images from each folder.\n",
            "Loading images for class ./curso_analise_imagens_satelite/FinalLixoes/Data//train/lixao\n",
            "Loading images for class ./curso_analise_imagens_satelite/FinalLixoes/Data//train/naolixao\n",
            "Training data loaded from the image sets: 352 ['lixao', 'naolixao']\n",
            "Do you want to save images as arrays for faster future loading? ((1:yes/0:no))\n",
            "Input:1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving data: ['lixao', 'naolixao'] ./curso_analise_imagens_satelite/FinalLixoes/Experiments//\n",
            "Training data loaded: 40 40 7 2 ['lixao', 'naolixao'] False\n",
            "Training and testing (0.8 - 0.2 split):1, training for predicting (no split, no testing): 2 or customized learning : 3 ? ((1/2/3))\n",
            "Input:1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-ce78e20e3d9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load or train main experiment (mines an dams discovery)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Discovery model:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmain_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_width\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnmean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_workflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2235380a897f>\u001b[0m in \u001b[0;36mmodel_workflow\u001b[0;34m(path_to_model, path_to_images, model_type, main_model, nmean, nmax)\u001b[0m\n\u001b[1;32m    737\u001b[0m     \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training data loaded:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_width\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNbands\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchar_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_width\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNbands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchar_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_width\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnmean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2235380a897f>\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m(path_to_model, model_type, image, label, image_width, image_height, n_classes, Nbands, clearMemory)\u001b[0m\n\u001b[1;32m    514\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m       \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_FCN_discovery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_width\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNbands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m       \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_FCN_oretype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_width\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNbands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-3a8770e737e0>\u001b[0m in \u001b[0;36mdefine_FCN_discovery\u001b[0;34m(image_width, image_height, nc, Nbands)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNbands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_augmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SAME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mx1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/merge.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(inputs, **kwargs)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m   \"\"\"\n\u001b[0;32m--> 791\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mAdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/merge.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m       raise ValueError(\n\u001b[0;32m---> 89\u001b[0;31m           \u001b[0;34m'A merge layer should be called on a list of inputs. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m           f'Received: input_shape={input_shape} (not a list of shapes)')\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: A merge layer should be called on a list of inputs. Received: input_shape=(None, None, None, 7) (not a list of shapes)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OddwSGSEkqL7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cdbd8095-ae50-4136-d46a-db0686e135e2"
      },
      "source": [
        "#load or train main experiment (mines an dams discovery)\n",
        "print(\"Discovery model:\")\n",
        "main_model,main_labels,image_width,image_height,nmean,nmax = model_workflow(path_to_output, path_to_images, 0)\n",
        "print(main_labels)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discovery model:\n",
            "Saved model found. Do you want to load it? ((1:yes/0:no))\n",
            "Input:0\n",
            "Saved training data found. Do you want to load it? ((1:yes/0:no))\n",
            "Input:0\n",
            "./curso_analise_imagens_satelite/FinalLixoes/Data//train/\n",
            "dir: ./curso_analise_imagens_satelite/FinalLixoes/Data//train/lixao 270\n",
            "dir: ./curso_analise_imagens_satelite/FinalLixoes/Data//train/naolixao 176\n",
            "Will load 176 images from each folder.\n",
            "Loading images for class ./curso_analise_imagens_satelite/FinalLixoes/Data//train/lixao\n",
            "Loading images for class ./curso_analise_imagens_satelite/FinalLixoes/Data//train/naolixao\n",
            "Training data loaded from the image sets: 352 ['lixao', 'naolixao']\n",
            "Do you want to save images as arrays for faster future loading? ((1:yes/0:no))\n",
            "Input:1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data: ['lixao', 'naolixao'] ./curso_analise_imagens_satelite/FinalLixoes/Experiments//\n",
            "Training data loaded: 40 40 7 2 ['lixao', 'naolixao'] True\n",
            "Training and testing (0.8 - 0.2 split):1, training for predicting (no split, no testing): 2 or customized learning : 3 ? ((1/2/3))\n",
            "Input:1\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, None, None, 7)]   0         \n",
            "                                                                 \n",
            " sequential_1 (Sequential)   (None, None, None, 7)     0         \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, None, None, 32)    2048      \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, None, None, 32)   0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, None, None, 64)    18496     \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, None, None, 64)   0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, None, None, 64)    36928     \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, None, None, 64)   0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, None, None, 64)    4160      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, None, None, 64)    0         \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, None, None, 2)     130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61,762\n",
            "Trainable params: 61,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6934 - custom_sparse_categorical_accuracy: 0.5037\n",
            "Epoch 1: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 2s 12ms/step - loss: 0.6935 - custom_sparse_categorical_accuracy: 0.4982\n",
            "Epoch 2/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6934 - custom_sparse_categorical_accuracy: 0.4963\n",
            "Epoch 2: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6934 - custom_sparse_categorical_accuracy: 0.4982\n",
            "Epoch 3/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6933 - custom_sparse_categorical_accuracy: 0.5074\n",
            "Epoch 3: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.6933 - custom_sparse_categorical_accuracy: 0.4982\n",
            "Epoch 4/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5037\n",
            "Epoch 4: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.4982\n",
            "Epoch 5/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6935 - custom_sparse_categorical_accuracy: 0.4375\n",
            "Epoch 5: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6935 - custom_sparse_categorical_accuracy: 0.4377\n",
            "Epoch 6/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.4926\n",
            "Epoch 6: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5018\n",
            "Epoch 7/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.4632\n",
            "Epoch 7: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.4591\n",
            "Epoch 8/20\n",
            "16/18 [=========================>....] - ETA: 0s - loss: 0.6931 - custom_sparse_categorical_accuracy: 0.5234\n",
            "Epoch 8: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5018\n",
            "Epoch 9/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6933 - custom_sparse_categorical_accuracy: 0.4559\n",
            "Epoch 9: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6933 - custom_sparse_categorical_accuracy: 0.4555\n",
            "Epoch 10/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 10: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.4875\n",
            "Epoch 11/20\n",
            "16/18 [=========================>....] - ETA: 0s - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 11: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.4982\n",
            "Epoch 12/20\n",
            "16/18 [=========================>....] - ETA: 0s - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.4883\n",
            "Epoch 12: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.4875\n",
            "Epoch 13/20\n",
            "14/18 [======================>.......] - ETA: 0s - loss: 0.6931 - custom_sparse_categorical_accuracy: 0.5179\n",
            "Epoch 13: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5018\n",
            "Epoch 14/20\n",
            "16/18 [=========================>....] - ETA: 0s - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5078\n",
            "Epoch 14: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5018\n",
            "Epoch 15/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 15: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5018\n",
            "Epoch 16/20\n",
            "16/18 [=========================>....] - ETA: 0s - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.4570\n",
            "Epoch 16: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.4626\n",
            "Epoch 17/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.4926\n",
            "Epoch 17: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 12ms/step - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5018\n",
            "Epoch 18/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 18: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5018\n",
            "Epoch 19/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 19: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5018\n",
            "Epoch 20/20\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 20: saving model to ./curso_analise_imagens_satelite/FinalLixoes/Experiments//trained_model/weights.ckpt\n",
            "18/18 [==============================] - 0s 13ms/step - loss: 0.6932 - custom_sparse_categorical_accuracy: 0.5018\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing:\n",
            "[[34  2]\n",
            " [30  5]]\n",
            "Precision:  0.7142857142857143\n",
            "Recall:  0.14285714285714285\n",
            "F1 score:  0.23809523809523808\n",
            "Mean accuracy:  0.5492957746478874\n",
            "AUC:  0.5436507936507936\n",
            "mcc:  0.14640921281248623\n",
            "kappa:  0.0882825040128411\n",
            "['lixao', 'naolixao']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ore type classifier\n",
        "#if oretypeclassifier is not None:  #check if ore classifier is required\n",
        "#  print(\"Ore classifier :\")\n",
        "#  ore_classifier,ore_labels,_,_,_,_ = model_workflow(path_to_oretype_output, path_to_oretype_samples, 1,main_model,nmean,nmax)\n",
        "#  print(ore_labels)\n",
        "#else:\n",
        "ore_classifier = None\n",
        "\n",
        "print(\"Model loaded. Options: (1: run model on images of prediction folder 2: Show models metrics 3: exit\")\n",
        "try:\n",
        "  x=int(input('Input:'))\n",
        "except ValueError:\n",
        "  print(\"Not a number\")\n",
        "\n",
        "if x==1:\n",
        "  # use model to predict\n",
        "  if os.path.exists(path_to_predict_images):\n",
        "    x = len(os.listdir(path_to_predict_images))\n",
        "    if x== 0:\n",
        "      print(\"No images found on the prediction folder\")\n",
        "    else:  \n",
        "      print(x,\"images found on the prediction folder. How many do you want to use?(-1 for all of them)\")\n",
        "      try:\n",
        "        n=int(input('Input:'))\n",
        "      except ValueError:\n",
        "        print(\"Not a number\")\n",
        "\n",
        "      print(\"Choose a slicing factor (use 1 for images up to 1GB, will break the TIFF in n^2 pieces)\")\n",
        "      try:\n",
        "        slicing=int(input('Input:'))\n",
        "      except ValueError:\n",
        "        print(\"Not a number\")\n",
        "\n",
        "      predict(main_model,ore_classifier,n,main_labels,ore_labels,image_width,image_height,nmean,nmax,N_BANDS,slicing)\n",
        "\n",
        "  else:\n",
        "    print(\"Folder containing images for prediction not found \",path_to_predict_images)\n",
        "elif x==2:\n",
        "  print(\"Main model metrics:\")\n",
        "  show_model_metrics(path_to_output)\n",
        "  print(\"Ore type model metrics:\")\n",
        "  show_model_metrics(path_to_oretype_output)"
      ],
      "metadata": {
        "id": "-MMY5XnGxinz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08A_y51oAL7I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl087ecWf2zI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}